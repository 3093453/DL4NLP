{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52aa545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b761c01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at redewiedergabe/bert-base-historical-german-rw-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"redewiedergabe/bert-base-historical-german-rw-cased\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"redewiedergabe/bert-base-historical-german-rw-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a0e7900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\",\n",
    "                                        output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "692ba238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ba49f068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1996, 3007, 1997, 2762, 2003,  103, 1012,  102]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(' The capital of Germany is [MASK]. ', return_tensors=\"pt\")\n",
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7af69cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 30522])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -6.5230,  -6.4974,  -6.5007,  ...,  -5.8824,  -5.6562,  -4.0490],\n",
       "         [-14.9857, -15.5860, -15.0506,  ..., -12.9823, -12.0289, -13.7651],\n",
       "         [ -9.9382, -10.5731,  -9.9997,  ...,  -8.9439,  -7.8317, -13.2562],\n",
       "         ...,\n",
       "         [ -4.2178,  -3.9710,  -3.8873,  ...,  -3.2657,  -4.3187,  -4.7271],\n",
       "         [-11.9945, -11.6814, -12.2533,  ...,  -9.3811, -10.9338,  -5.2599],\n",
       "         [-11.4986, -11.5364, -11.6248,  ...,  -8.9861, -10.8496,  -7.1109]]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # LOGITS is an attribute of the model output, not a function performed on the output\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "print(logits.shape)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "faf82579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bbcaee08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19349])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "predicted_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "51132cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b o n n'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(19349)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f880736",
   "metadata": {},
   "source": [
    "# Try to get embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9f21fd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
    "       \"fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ad9fd03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "# BERT can classify two sentences, so we need to distinguish them.\n",
    "# If only a single sentence is passed, use array of 1s\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "33ab3c9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2044, 11065,  2769,  2013,  1996,  2924, 11632,  1010,  1996,\n",
      "          2924, 27307,  2001,  2464,  5645,  2006,  1996,  5900,  2314,  2924,\n",
      "          1012,   102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens]); print(tokens_tensor)\n",
    "segments_tensors = torch.tensor([segments_ids]); print(segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9ae2d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put model in feed forward mode\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "085367f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -8.6172,  -8.5317,  -8.4705,  ...,  -8.0710,  -7.8490,  -5.1670],\n",
       "         [ -8.9969,  -8.8552,  -9.0684,  ...,  -9.0791,  -8.2523,  -7.0004],\n",
       "         [ -7.0942,  -6.8015,  -7.1649,  ...,  -7.0118,  -5.0229,  -7.4184],\n",
       "         ...,\n",
       "         [-12.3700, -12.2159, -12.1982,  ..., -11.8907, -10.9755, -10.3638],\n",
       "         [-14.1743, -13.7497, -13.8875,  ..., -10.8686, -12.0782, -11.4451],\n",
       "         [-13.6082, -13.6974, -14.0924,  ..., -14.4759, -11.5256, -10.6159]]]), hidden_states=(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "         [ 0.2329,  0.1390,  0.2979,  ..., -0.0655,  0.8885,  0.5109],\n",
       "         [ 0.2257, -0.7165, -0.7255,  ...,  0.4844,  0.6030, -0.0957],\n",
       "         ...,\n",
       "         [-0.0374, -0.6155, -1.4419,  ...,  0.0793, -0.0811, -0.3802],\n",
       "         [-0.0228,  0.4207, -0.3288,  ...,  0.4464,  0.5178,  0.5501],\n",
       "         [-0.2350,  0.1566, -0.0462,  ..., -0.4206,  0.3074, -0.2288]]]), tensor([[[ 0.0522,  0.0595, -0.2179,  ...,  0.2280, -0.0712,  0.0148],\n",
       "         [ 0.3819,  0.1475,  0.2414,  ...,  0.3397,  0.7607,  0.4999],\n",
       "         [ 0.1705, -0.6168, -0.7296,  ...,  0.8631,  0.6274, -0.3727],\n",
       "         ...,\n",
       "         [ 0.6982, -0.4554, -1.7845,  ...,  0.3308,  0.0710, -0.5187],\n",
       "         [-0.0905,  0.1862, -0.4437,  ...,  0.2244,  0.1810,  0.3740],\n",
       "         [-0.0825,  0.0466, -0.1526,  ..., -0.2033,  0.3370, -0.1767]]]), tensor([[[-0.0357, -0.2022, -0.4103,  ...,  0.2511,  0.0586, -0.0547],\n",
       "         [ 0.1430,  0.0747,  0.0595,  ..., -0.2914,  0.1733,  0.4265],\n",
       "         [ 0.5752, -1.2385, -0.5650,  ...,  0.8995,  0.4652, -0.8080],\n",
       "         ...,\n",
       "         [ 0.9157, -0.4118, -1.6042,  ...,  0.1454,  0.1699, -0.2230],\n",
       "         [-0.1816,  0.0099,  0.0370,  ..., -0.2178,  0.0481,  0.3477],\n",
       "         [-0.1524, -0.1068, -0.0868,  ..., -0.1369,  0.2633, -0.3754]]]), tensor([[[-0.0183, -0.3853, -0.1600,  ...,  0.2446,  0.2160,  0.0633],\n",
       "         [ 0.7727, -0.3786,  0.6677,  ..., -0.2014,  0.0094,  0.6461],\n",
       "         [ 0.8822, -0.7807, -0.6305,  ...,  0.5223,  0.3687, -0.9875],\n",
       "         ...,\n",
       "         [ 0.9788, -0.0669, -1.6344,  ...,  0.0341,  0.0437, -0.1636],\n",
       "         [-0.4724, -0.0258,  0.3107,  ..., -0.2553, -0.1049,  0.0897],\n",
       "         [-0.0816, -0.1028,  0.0947,  ...,  0.0091,  0.0649, -0.0646]]]), tensor([[[ 0.0207, -0.3990, -0.8255,  ...,  0.3448,  0.0951,  0.3650],\n",
       "         [ 0.5667, -0.7513,  0.0695,  ..., -0.1052, -0.1700,  0.4333],\n",
       "         [ 0.7095, -0.2044, -0.1709,  ...,  0.7092,  0.0542, -1.0380],\n",
       "         ...,\n",
       "         [ 1.3519,  0.3352, -1.3555,  ...,  0.1676, -0.2817,  0.1042],\n",
       "         [-0.2488,  0.1044, -0.1778,  ..., -0.2713, -0.2643,  0.2310],\n",
       "         [-0.0266, -0.0469,  0.0053,  ...,  0.0085,  0.0417, -0.0486]]]), tensor([[[-0.3443, -0.6315, -0.6647,  ...,  0.0050,  0.1167,  0.3614],\n",
       "         [ 0.8173, -0.9999, -0.1752,  ..., -0.4932, -0.2556,  0.2038],\n",
       "         [ 0.8973, -0.2652, -0.6559,  ...,  0.5725,  0.4178, -0.6356],\n",
       "         ...,\n",
       "         [ 1.5582,  0.4479, -1.2054,  ...,  0.2052, -0.5083,  0.3994],\n",
       "         [-0.1898,  0.0898, -0.2766,  ..., -0.3044, -0.4370,  0.4248],\n",
       "         [-0.0264, -0.0364,  0.0304,  ...,  0.0160,  0.0067, -0.0486]]]), tensor([[[-2.3737e-01, -9.9507e-01, -4.5364e-01,  ..., -2.1456e-01,\n",
       "           3.5440e-01,  2.5447e-01],\n",
       "         [ 5.7495e-01, -9.1262e-01, -2.3175e-01,  ..., -1.8444e-01,\n",
       "          -1.8389e-01,  1.3615e-01],\n",
       "         [ 2.5019e-01, -5.6334e-01, -7.0052e-01,  ...,  2.0831e-01,\n",
       "           6.6902e-01,  5.5974e-02],\n",
       "         ...,\n",
       "         [ 1.5415e+00,  3.5870e-03, -1.0563e+00,  ...,  6.0117e-02,\n",
       "          -3.6085e-01,  5.3854e-01],\n",
       "         [-5.8238e-01,  8.6806e-02, -4.4414e-01,  ..., -8.1630e-01,\n",
       "          -4.2633e-01,  4.1039e-01],\n",
       "         [ 1.3974e-02, -5.6850e-02, -4.0642e-03,  ...,  7.8790e-05,\n",
       "          -1.4510e-02, -4.7457e-02]]]), tensor([[[-1.5213e-01, -9.7455e-01, -5.6574e-01,  ..., -6.0024e-01,\n",
       "           3.0606e-01,  3.6931e-01],\n",
       "         [ 2.7012e-01, -5.4540e-01,  1.6653e-02,  ...,  6.2675e-02,\n",
       "           2.6824e-01,  3.4129e-02],\n",
       "         [-4.1255e-01, -6.4567e-01, -2.0088e-01,  ...,  2.3458e-01,\n",
       "           1.4713e-01,  4.0941e-03],\n",
       "         ...,\n",
       "         [ 1.3217e+00, -6.6628e-02, -8.1355e-01,  ..., -3.9645e-01,\n",
       "          -5.2720e-01,  2.0277e-01],\n",
       "         [-5.6073e-01, -4.5253e-01, -5.8201e-01,  ..., -1.4335e+00,\n",
       "           2.8755e-01,  5.9470e-01],\n",
       "         [ 6.2754e-05, -2.2453e-02, -2.1868e-02,  ..., -1.4897e-02,\n",
       "           2.8922e-03, -4.3436e-02]]]), tensor([[[ 0.0967, -0.8567, -0.5056,  ..., -0.4204,  0.1267,  0.3225],\n",
       "         [-0.0109, -0.2976,  0.2986,  ..., -0.3344,  0.2755, -0.1571],\n",
       "         [-0.6171, -0.4909, -0.1630,  ..., -0.3340,  0.4452, -0.2641],\n",
       "         ...,\n",
       "         [ 0.9738,  0.0119, -0.6562,  ..., -0.3114, -0.1033,  0.1980],\n",
       "         [-0.8265, -0.4334, -0.7586,  ..., -1.2721,  0.0482,  0.3751],\n",
       "         [ 0.0196,  0.0017,  0.0323,  ..., -0.0348, -0.0410, -0.0743]]]), tensor([[[-0.3495, -0.7788, -0.7701,  ..., -0.0715,  0.2350,  0.1686],\n",
       "         [-0.2694, -0.3884, -0.3181,  ..., -0.7334,  0.0196, -0.4148],\n",
       "         [-0.5444, -0.3546, -0.2131,  ..., -0.5306,  0.2799, -0.3587],\n",
       "         ...,\n",
       "         [ 0.6356,  0.0980, -0.2775,  ..., -0.6243, -0.2620,  0.1343],\n",
       "         [-0.4167,  0.1132, -0.4761,  ..., -0.5415, -0.1268,  0.0483],\n",
       "         [-0.0671, -0.0451,  0.0232,  ..., -0.0718, -0.0020, -0.0556]]]), tensor([[[-0.9560, -1.0372, -0.8875,  ...,  0.0905, -0.3867,  0.3258],\n",
       "         [-0.3215, -1.1213, -0.2349,  ..., -0.5940,  0.1851, -0.5076],\n",
       "         [-0.4795, -1.0745, -0.1744,  ..., -0.5026,  0.1293, -0.1963],\n",
       "         ...,\n",
       "         [ 0.2736, -0.4021, -0.1525,  ..., -0.7099, -0.7498, -0.0066],\n",
       "         [-0.0143,  0.0187, -0.0490,  ..., -0.0197, -0.0281,  0.0165],\n",
       "         [-0.5765, -0.5547, -0.2122,  ...,  0.2610, -0.4111, -0.1223]]]), tensor([[[-0.6121, -0.6371, -0.8917,  ...,  0.1026, -0.2241,  0.3330],\n",
       "         [-0.2817, -0.6142, -0.4499,  ..., -0.7273,  0.0304, -0.5106],\n",
       "         [-0.4519, -0.2627, -0.1917,  ..., -0.5405, -0.2146, -0.2140],\n",
       "         ...,\n",
       "         [ 0.3213, -0.2997, -0.0471,  ..., -0.8094, -0.7861, -0.0618],\n",
       "         [ 0.0430,  0.0219, -0.0214,  ...,  0.0196, -0.0353,  0.0072],\n",
       "         [-0.0599, -0.6397, -0.6008,  ...,  0.4218, -0.5170, -0.1616]]]), tensor([[[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
       "         [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
       "         [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
       "         ...,\n",
       "         [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
       "         [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
       "         [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]]])), attentions=None)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put model in feed forward mode\n",
    "model.eval();\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    \n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0117e866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "torch.Size([1, 22, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "         [ 0.2329,  0.1390,  0.2979,  ..., -0.0655,  0.8885,  0.5109],\n",
       "         [ 0.2257, -0.7165, -0.7255,  ...,  0.4844,  0.6030, -0.0957],\n",
       "         ...,\n",
       "         [-0.0374, -0.6155, -1.4419,  ...,  0.0793, -0.0811, -0.3802],\n",
       "         [-0.0228,  0.4207, -0.3288,  ...,  0.4464,  0.5178,  0.5501],\n",
       "         [-0.2350,  0.1566, -0.0462,  ..., -0.4206,  0.3074, -0.2288]]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = outputs.hidden_states\n",
    "print(len(hidden_states)) # (initial embeddings + 12 BERT layers)\n",
    "print(hidden_states[0].shape)\n",
    "hidden_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7e6b005d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 8.1731e-01, -9.9992e-01, -1.7517e-01,  1.2036e-01,  2.9562e-01,\n",
       "         5.9975e-01,  1.1497e+00, -5.6179e-01, -3.8268e-01, -4.4418e-01,\n",
       "         2.9177e-02, -5.5862e-01, -1.3647e-01, -8.3546e-01,  6.0544e-01,\n",
       "        -1.4141e-01,  1.8004e-01,  1.3629e-01, -1.1580e+00, -8.6062e-02,\n",
       "         1.0039e+00, -1.2701e+00, -6.5069e-01,  5.4862e-01, -4.5827e-02,\n",
       "         4.8976e-01, -1.1270e-01, -2.5345e-01, -7.8597e-02,  9.2477e-01,\n",
       "        -7.5478e-02, -7.2561e-01, -2.5935e-01,  2.5833e-01, -3.5075e-01,\n",
       "         1.1993e-01, -2.1003e-01, -1.8066e-01, -9.5722e-02,  7.0053e-01,\n",
       "         5.5000e-01,  1.8216e-01, -1.3134e+00,  8.6298e-01, -5.3230e-02,\n",
       "        -1.3323e-01, -1.0146e-02,  5.8360e-01,  3.6888e-01, -5.2650e-01,\n",
       "        -1.0158e+00,  4.5717e-01,  5.3313e-01, -5.5089e-01, -1.0781e+00,\n",
       "        -7.9238e-01, -1.3952e-02, -5.8646e-01,  7.8767e-01,  6.3859e-02,\n",
       "         3.1624e-01,  4.9102e-01,  5.8417e-01, -6.8979e-01,  1.2734e-03,\n",
       "        -6.0200e-01, -4.1721e-01, -2.6369e-01,  5.9095e-01,  5.4801e-01,\n",
       "         3.3939e-01,  4.0964e-01, -8.8157e-02, -2.1182e-02,  4.8044e-01,\n",
       "         1.1368e+00, -7.6147e-01, -8.7117e-02,  2.3628e-01,  3.9444e-01,\n",
       "         2.5737e-01,  5.9055e-01, -1.1803e+00, -3.2307e-01, -6.8083e-02,\n",
       "         6.6241e-02, -5.2815e-02, -1.3704e-01,  6.0285e-02,  1.8441e-02,\n",
       "        -1.4120e+00, -8.1415e-01,  1.0548e-01,  5.9486e-01,  1.9205e-01,\n",
       "        -3.7823e-01,  3.7931e-03, -3.0897e-01,  4.3148e-01, -1.2075e+00,\n",
       "        -1.6225e-01,  4.4084e-01, -1.0997e+00, -9.9095e-02, -9.8662e-02,\n",
       "        -3.1382e-01, -5.3319e-01,  6.0194e-01, -2.9546e-01,  3.2319e-01,\n",
       "         6.6693e-01,  5.8606e-01,  4.7939e-01, -1.2205e-01, -4.2232e-01,\n",
       "        -8.1717e-01,  5.6819e-01,  5.6297e-01,  6.4814e-02,  3.8485e-01,\n",
       "        -9.2599e-03, -2.3918e-01, -7.5725e-01, -8.2418e-02, -5.8484e-01,\n",
       "        -6.2078e-01, -1.2420e-01, -5.3235e-01,  8.2525e-01, -2.2401e-01,\n",
       "        -1.0223e+00,  3.2156e-01,  6.1546e-01, -5.8470e-01, -1.2034e+00,\n",
       "         6.8627e-01,  9.1552e-02, -1.8995e-01, -7.4306e-01, -2.9995e-01,\n",
       "        -1.0617e+00, -4.0814e-01,  1.7031e+00, -1.0691e+00,  8.3911e-02,\n",
       "        -8.3975e-01,  2.0860e-01,  3.3559e-01, -1.3674e+00,  6.6761e-01,\n",
       "        -5.5000e-01, -5.8822e-02, -7.7254e-01, -1.0282e+00,  5.1284e-01,\n",
       "        -7.7022e-02, -1.4053e-01, -1.7472e-01,  3.0028e-01,  6.7481e-02,\n",
       "         6.9766e-01,  1.2767e+00, -1.0297e-01, -5.4028e-01, -9.4696e-01,\n",
       "        -4.7263e-01, -5.4489e-01, -7.8444e-01, -3.5850e-01,  3.4801e-01,\n",
       "        -4.3200e-01, -2.6508e-01, -4.3920e-01, -7.9816e-01, -2.7153e-02,\n",
       "        -9.7257e-02,  3.0873e-02, -3.9007e-01, -2.8940e-01, -2.1405e-01,\n",
       "         3.8967e-01,  1.2235e-01, -1.0139e+00, -3.2571e-01, -3.7574e-02,\n",
       "        -3.9459e-03, -3.5134e-01, -1.3239e-01,  2.0933e-01, -1.2395e-01,\n",
       "         4.1865e-01, -9.3526e-02, -2.1281e-02,  1.8161e-02,  6.1040e-01,\n",
       "         1.0380e-01, -4.6553e-01, -1.6140e-01, -6.4210e-01,  8.1845e-02,\n",
       "        -1.0600e+00,  5.7846e-01,  6.4568e-01,  2.9145e-01,  2.2080e-01,\n",
       "         8.2520e-01, -3.9089e-01,  8.3337e-01, -3.2859e-01,  4.5298e-01,\n",
       "         1.9363e-01, -3.0176e-03, -3.7762e-01,  3.8687e-02, -4.9393e-01,\n",
       "        -3.0789e-01,  2.6315e-01, -1.1490e-01,  5.0020e-02, -2.6244e-02,\n",
       "         5.2858e-01,  1.3392e-01,  1.6494e-01,  7.1267e-01,  8.8590e-02,\n",
       "         2.8997e-01, -9.7011e-01,  6.9106e-01,  5.9941e-01, -1.0668e+00,\n",
       "        -1.1313e-01,  6.1658e-01,  8.6042e-01,  1.0803e+00, -2.6352e-01,\n",
       "         1.1087e-01, -9.4258e-01,  1.6976e-03,  3.0802e-01, -1.4508e-01,\n",
       "        -3.9565e-01,  7.5705e-01, -8.3466e-02,  2.5843e-01,  3.9742e-01,\n",
       "        -6.3774e-02, -1.3594e-01, -3.4376e-01, -1.0350e+00, -1.4170e-01,\n",
       "        -2.0543e-01,  1.6199e-01, -1.0393e-01, -1.6082e+00,  2.1623e-01,\n",
       "        -4.5892e-01, -2.0409e-01,  6.7619e-01, -2.4858e-01, -6.5064e-01,\n",
       "         3.6943e-01, -3.4683e-01,  1.0171e+00,  5.9748e-01,  1.7504e-01,\n",
       "        -7.7479e-01,  7.1670e-01, -9.1875e-02, -6.0417e-02, -6.3863e-01,\n",
       "         1.4068e-01, -4.7363e-01,  8.2344e-01,  2.0416e-01, -5.5222e-02,\n",
       "        -1.0243e+00, -6.9113e-02,  5.3806e-01, -6.7265e-02, -3.8315e-01,\n",
       "         6.9953e-01, -1.1162e+00,  6.7091e-01, -3.4398e-03,  4.3205e-01,\n",
       "        -7.9183e-02,  2.7027e-01,  5.6888e-01, -4.8376e-03,  4.5531e-02,\n",
       "         6.1716e-01,  2.9088e-02, -1.8845e+00, -1.0204e-01, -1.4656e-01,\n",
       "        -1.2282e-01,  6.9568e-01,  3.6761e-01, -5.3763e-01,  5.4727e-01,\n",
       "         1.4592e+00, -6.7763e-01, -1.0915e+00,  5.9226e-01,  2.7786e-01,\n",
       "        -7.9332e-02,  5.2279e-01,  6.9944e-01, -1.1627e+01, -1.2476e-01,\n",
       "         7.7907e-01, -7.8560e-01,  7.7636e-02, -2.7761e-01,  1.1253e+00,\n",
       "        -1.9089e-01,  1.1061e-01,  8.3675e-01,  5.6172e-01,  2.0829e-01,\n",
       "        -3.5807e-01, -2.2577e-01, -1.5077e-01, -4.2020e-01,  5.3074e-01,\n",
       "         3.2667e-02,  4.3071e-01, -4.8084e-01,  7.7960e-01,  6.9603e-01,\n",
       "        -6.5760e-01, -2.5594e-01, -1.1315e+00,  5.8363e-01, -4.7872e-01,\n",
       "         9.5055e-01,  9.1463e-01,  9.0327e-01,  5.0433e-02, -9.8453e-01,\n",
       "        -1.1033e+00,  2.7195e+00, -8.8030e-02,  1.9712e-01, -6.9747e-02,\n",
       "        -3.0441e-01,  5.1144e-01, -1.1914e-01,  3.6891e-01, -2.9024e-01,\n",
       "        -8.4368e-02, -4.2141e-01,  1.1594e+00,  2.8425e-01,  3.7920e-01,\n",
       "         9.5616e-01, -3.6158e-01, -1.9019e-01, -7.0381e-01,  3.2741e-01,\n",
       "        -1.4165e-01, -2.3003e-02,  8.5692e-01,  8.2670e-01,  4.2924e-02,\n",
       "         1.0693e+00,  2.9252e-01, -1.1487e+00, -3.7429e-01,  3.2266e-01,\n",
       "        -2.6329e-01, -5.5185e-01, -1.0532e-01, -1.2469e-01,  9.5573e-02,\n",
       "         4.1476e-01,  1.2682e-03,  5.3676e-02,  5.3275e-01, -7.4372e-03,\n",
       "        -6.2946e-01, -1.3566e+00, -5.5887e-02,  2.4598e-01,  9.0590e-01,\n",
       "        -2.4161e-01,  2.4582e-01,  1.1686e+00, -9.7725e-01, -6.6054e-01,\n",
       "         2.2994e-01, -5.0064e-01,  1.9763e-01,  1.0954e-01,  4.5585e-01,\n",
       "         7.3007e-01, -7.1417e-01,  3.9019e-01, -8.4151e-01,  1.3864e+00,\n",
       "        -7.1909e-01,  6.8850e-01, -3.5340e-01,  7.9864e-01, -7.6272e-03,\n",
       "        -5.4710e-02,  5.8276e-01,  2.9813e-01,  2.5023e-01, -6.7598e-01,\n",
       "         2.3803e-01,  8.3396e-01,  5.8735e-01, -4.4038e-01, -3.4871e-01,\n",
       "         5.8938e-01,  7.4093e-01,  7.6126e-01, -5.2462e-01,  7.6493e-03,\n",
       "         3.0805e-02, -4.3132e-01,  2.9652e-01,  8.8549e-01, -3.7277e-01,\n",
       "        -6.3983e-01,  1.3645e-01, -5.9022e-01,  2.9000e-01, -5.1428e-01,\n",
       "         2.5329e-01, -5.2276e-01,  8.3320e-01,  2.0501e-01,  4.8414e-01,\n",
       "         9.8661e-01, -7.0248e-01,  2.7713e-02, -9.4580e-01, -3.8230e-01,\n",
       "         2.0451e-03,  6.9145e-01,  2.7147e-01, -2.9368e-01, -3.2987e-01,\n",
       "        -8.7564e-01,  9.3371e-01,  9.4046e-01,  3.8199e-01, -4.3253e-01,\n",
       "        -1.5863e-01,  5.7383e-01,  1.0033e-01, -2.9014e-01,  4.9104e-01,\n",
       "         1.0821e+00,  9.4522e-01, -3.9635e-01, -3.7073e-01,  4.8034e-01,\n",
       "         6.5090e-01, -8.3357e-01, -2.6655e-01, -8.1239e-01,  1.1731e-01,\n",
       "         6.0353e-01,  3.1508e-01, -3.0835e-01,  4.9033e-01,  4.4152e-01,\n",
       "        -1.9095e-01, -8.3411e-01, -2.4591e-01,  7.9199e-02, -4.1396e-01,\n",
       "         7.4463e-02, -5.5281e-01, -5.4643e-01,  5.1661e-01,  9.6344e-01,\n",
       "         6.3384e-01, -3.3788e-01, -6.9693e-01, -1.5636e-01,  4.0012e-01,\n",
       "         8.3889e-02, -2.8315e-01, -1.9845e-01, -5.1275e-01,  1.0516e+00,\n",
       "        -8.8288e-01,  1.4571e+00, -1.0551e-01,  7.4202e-01, -7.8496e-01,\n",
       "         7.0497e-01, -3.4344e-01, -2.2912e-01,  5.1968e-01,  2.3711e-01,\n",
       "        -1.2532e-01,  1.1823e-01, -9.1844e-02, -9.9738e-02,  1.2088e-01,\n",
       "         4.9716e-01, -2.2694e-01,  1.3014e-01, -5.3598e-01,  9.2713e-01,\n",
       "         1.0331e+00, -5.6596e-02,  8.6941e-01, -2.3346e-01, -9.1799e-01,\n",
       "         8.2636e-02, -1.0723e-01, -2.9205e-01, -6.0483e-01,  5.3401e-01,\n",
       "        -3.4245e-01,  2.6196e-01, -4.2500e-01, -4.5592e-01, -6.3151e-01,\n",
       "        -8.1865e-01,  1.8080e-01,  5.6503e-01, -4.8046e-01,  7.0044e-01,\n",
       "         3.4980e-01,  6.3936e-02, -3.3304e-01,  4.6375e-01,  1.8631e-01,\n",
       "        -7.7979e-01, -1.4356e-02, -2.5815e-01,  8.8409e-01, -2.3982e+00,\n",
       "         6.2596e-02,  2.4742e-01, -4.0780e-01, -7.1125e-01,  7.2434e-01,\n",
       "        -9.6506e-01,  5.2342e-01,  6.1625e-01,  3.8829e-01, -2.4160e-01,\n",
       "        -1.5472e-02, -1.9383e-01,  9.5845e-01, -1.2149e-01, -1.2103e-01,\n",
       "        -5.9716e-01, -6.4852e-02, -5.1265e-01,  1.3912e+00, -5.8276e-02,\n",
       "         1.4866e+00, -2.0587e-01,  6.2514e-01, -6.5764e-01, -8.2499e-02,\n",
       "        -7.5435e-01,  4.6195e-02,  2.7278e-01, -8.9752e-01, -2.8875e-01,\n",
       "         2.4083e-01, -5.8621e-01, -3.5956e-01, -2.1002e-01,  6.7226e-01,\n",
       "        -3.4852e-01, -1.8022e-01, -1.5838e-01, -6.3288e-01,  2.1204e-01,\n",
       "         1.2241e-01,  6.6224e-02, -1.4064e-01,  9.0484e-01,  1.0722e+00,\n",
       "        -3.4264e-01, -1.0032e+00,  3.3641e-01,  7.3450e-01,  3.9000e-01,\n",
       "        -2.4355e-01, -4.7346e-01, -5.2580e-01,  4.1339e-01,  4.7912e-01,\n",
       "         8.2908e-01, -2.1970e-01, -2.6961e-01,  3.5319e-01,  1.3073e+00,\n",
       "         9.4340e-01,  1.1930e-01,  1.1647e+00,  1.2416e+00,  2.3687e-01,\n",
       "         5.0048e-01, -4.4417e-01, -4.3608e-01, -2.9818e-01,  3.3911e-01,\n",
       "        -6.2036e-01,  2.5560e-02, -1.1458e-01, -4.1155e-01, -4.1265e-01,\n",
       "        -7.4291e-01,  1.2265e+00, -8.1281e-01,  7.4763e-01,  9.6777e-01,\n",
       "        -7.3752e-01, -1.8207e-02,  1.1790e-01,  2.2965e-01,  3.1769e-01,\n",
       "         7.9403e-01, -2.8413e-01, -3.9872e-01, -2.1024e-02, -8.7116e-01,\n",
       "        -3.1778e-01,  2.3517e-01, -1.1155e+00,  8.7081e-01, -1.1295e-01,\n",
       "         8.6454e-02, -8.0450e-01, -1.0169e+00,  9.9823e-02,  2.6142e-01,\n",
       "        -2.0695e-02, -4.2987e-02, -7.6084e-01, -5.0758e-01, -1.4298e+00,\n",
       "         3.7476e-01,  7.4897e-01, -1.1090e+00,  3.3059e-01,  6.7386e-01,\n",
       "        -7.2207e-01, -2.7645e-01,  2.0773e-01,  3.4298e-01,  6.0275e-03,\n",
       "         2.1116e-01, -1.7298e-01,  4.2990e-02,  3.9131e-01, -7.2789e-02,\n",
       "         9.7465e-01, -8.8257e-01,  5.7365e-01,  2.2567e-01, -1.9834e-01,\n",
       "        -5.6455e-01,  7.7716e-01, -1.9070e-01,  1.1402e-02, -4.1875e-01,\n",
       "         5.2293e-02, -1.8527e-01,  2.6182e-01, -6.6724e-01,  3.6443e-01,\n",
       "        -5.9352e-02,  8.8659e-01,  4.3296e-01,  5.2584e-01,  2.0916e-01,\n",
       "        -9.2825e-03,  5.5042e-02, -7.8891e-01, -3.6496e-01,  3.4676e-01,\n",
       "        -3.5044e-01,  1.1357e+00,  2.3058e-01,  5.0044e-01,  1.8054e+00,\n",
       "        -4.5294e-01, -8.2493e-01,  6.0818e-01, -2.2388e-01,  7.2998e-02,\n",
       "        -2.5038e-01, -5.4092e-01, -1.0346e-02, -3.1982e-01,  2.7023e-01,\n",
       "         6.5058e-01, -3.5628e-01, -2.7938e-01, -1.8869e-01, -4.5419e-01,\n",
       "        -1.8278e-01, -7.7077e-01, -3.6342e-01,  6.0602e-02,  1.2846e-01,\n",
       "         4.0602e-01, -4.9566e-01, -7.4922e-01, -6.2417e-01, -4.8874e-01,\n",
       "         7.4626e-02, -1.0925e+00, -2.0911e-01,  4.5625e-01, -3.6018e-01,\n",
       "        -6.5350e-01, -9.7127e-02, -3.3654e-01, -1.1228e-01,  5.3030e-01,\n",
       "        -4.2966e-01, -2.6581e-02, -8.5774e-01,  1.3100e-01,  4.5229e-01,\n",
       "        -1.6558e-01,  5.7636e-01,  6.8195e-01, -9.4030e-01, -4.1844e-01,\n",
       "        -5.6694e-01, -5.1637e-01, -6.1412e-01, -7.9490e-01, -2.2845e-01,\n",
       "        -2.9629e-01, -3.4522e-01,  7.2599e-01, -5.2507e-01,  7.2299e-02,\n",
       "         1.1828e+00, -6.7103e-02,  1.4449e-01, -3.5617e-01,  7.2179e-01,\n",
       "         1.1163e-01,  8.7825e-02,  3.2257e-01, -5.3663e-01, -2.0105e+00,\n",
       "         1.7820e-01, -4.6249e-01,  8.5983e-01,  1.6226e+00, -9.6453e-01,\n",
       "        -3.2725e-01,  5.6090e-01,  1.3626e+00, -2.2333e-01, -2.5919e-02,\n",
       "        -4.9317e-01, -2.5557e-01,  2.0377e-01])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 5 # layer of the network\n",
    "batch = 0 # basically always zero or 1! since we only have one or two sentences\n",
    "token = 1 \n",
    "vec = hidden_states[layer][batch][token]; print(vec.shape)\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "397c00c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a6d3b7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 30522])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape # this we dont want, its the activation for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "68b8b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Current dimensions:\n",
    "#[# layers, # batches, # tokens, # features]\n",
    "\n",
    "#Desired dimensions:\n",
    "#[# tokens, # layers, # features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "48a9b053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "          [ 0.2329,  0.1390,  0.2979,  ..., -0.0655,  0.8885,  0.5109],\n",
       "          [ 0.2257, -0.7165, -0.7255,  ...,  0.4844,  0.6030, -0.0957],\n",
       "          ...,\n",
       "          [-0.0374, -0.6155, -1.4419,  ...,  0.0793, -0.0811, -0.3802],\n",
       "          [-0.0228,  0.4207, -0.3288,  ...,  0.4464,  0.5178,  0.5501],\n",
       "          [-0.2350,  0.1566, -0.0462,  ..., -0.4206,  0.3074, -0.2288]]]),\n",
       " tensor([[[ 0.0522,  0.0595, -0.2179,  ...,  0.2280, -0.0712,  0.0148],\n",
       "          [ 0.3819,  0.1475,  0.2414,  ...,  0.3397,  0.7607,  0.4999],\n",
       "          [ 0.1705, -0.6168, -0.7296,  ...,  0.8631,  0.6274, -0.3727],\n",
       "          ...,\n",
       "          [ 0.6982, -0.4554, -1.7845,  ...,  0.3308,  0.0710, -0.5187],\n",
       "          [-0.0905,  0.1862, -0.4437,  ...,  0.2244,  0.1810,  0.3740],\n",
       "          [-0.0825,  0.0466, -0.1526,  ..., -0.2033,  0.3370, -0.1767]]]),\n",
       " tensor([[[-0.0357, -0.2022, -0.4103,  ...,  0.2511,  0.0586, -0.0547],\n",
       "          [ 0.1430,  0.0747,  0.0595,  ..., -0.2914,  0.1733,  0.4265],\n",
       "          [ 0.5752, -1.2385, -0.5650,  ...,  0.8995,  0.4652, -0.8080],\n",
       "          ...,\n",
       "          [ 0.9157, -0.4118, -1.6042,  ...,  0.1454,  0.1699, -0.2230],\n",
       "          [-0.1816,  0.0099,  0.0370,  ..., -0.2178,  0.0481,  0.3477],\n",
       "          [-0.1524, -0.1068, -0.0868,  ..., -0.1369,  0.2633, -0.3754]]]),\n",
       " tensor([[[-0.0183, -0.3853, -0.1600,  ...,  0.2446,  0.2160,  0.0633],\n",
       "          [ 0.7727, -0.3786,  0.6677,  ..., -0.2014,  0.0094,  0.6461],\n",
       "          [ 0.8822, -0.7807, -0.6305,  ...,  0.5223,  0.3687, -0.9875],\n",
       "          ...,\n",
       "          [ 0.9788, -0.0669, -1.6344,  ...,  0.0341,  0.0437, -0.1636],\n",
       "          [-0.4724, -0.0258,  0.3107,  ..., -0.2553, -0.1049,  0.0897],\n",
       "          [-0.0816, -0.1028,  0.0947,  ...,  0.0091,  0.0649, -0.0646]]]),\n",
       " tensor([[[ 0.0207, -0.3990, -0.8255,  ...,  0.3448,  0.0951,  0.3650],\n",
       "          [ 0.5667, -0.7513,  0.0695,  ..., -0.1052, -0.1700,  0.4333],\n",
       "          [ 0.7095, -0.2044, -0.1709,  ...,  0.7092,  0.0542, -1.0380],\n",
       "          ...,\n",
       "          [ 1.3519,  0.3352, -1.3555,  ...,  0.1676, -0.2817,  0.1042],\n",
       "          [-0.2488,  0.1044, -0.1778,  ..., -0.2713, -0.2643,  0.2310],\n",
       "          [-0.0266, -0.0469,  0.0053,  ...,  0.0085,  0.0417, -0.0486]]]),\n",
       " tensor([[[-0.3443, -0.6315, -0.6647,  ...,  0.0050,  0.1167,  0.3614],\n",
       "          [ 0.8173, -0.9999, -0.1752,  ..., -0.4932, -0.2556,  0.2038],\n",
       "          [ 0.8973, -0.2652, -0.6559,  ...,  0.5725,  0.4178, -0.6356],\n",
       "          ...,\n",
       "          [ 1.5582,  0.4479, -1.2054,  ...,  0.2052, -0.5083,  0.3994],\n",
       "          [-0.1898,  0.0898, -0.2766,  ..., -0.3044, -0.4370,  0.4248],\n",
       "          [-0.0264, -0.0364,  0.0304,  ...,  0.0160,  0.0067, -0.0486]]]),\n",
       " tensor([[[-2.3737e-01, -9.9507e-01, -4.5364e-01,  ..., -2.1456e-01,\n",
       "            3.5440e-01,  2.5447e-01],\n",
       "          [ 5.7495e-01, -9.1262e-01, -2.3175e-01,  ..., -1.8444e-01,\n",
       "           -1.8389e-01,  1.3615e-01],\n",
       "          [ 2.5019e-01, -5.6334e-01, -7.0052e-01,  ...,  2.0831e-01,\n",
       "            6.6902e-01,  5.5974e-02],\n",
       "          ...,\n",
       "          [ 1.5415e+00,  3.5870e-03, -1.0563e+00,  ...,  6.0117e-02,\n",
       "           -3.6085e-01,  5.3854e-01],\n",
       "          [-5.8238e-01,  8.6806e-02, -4.4414e-01,  ..., -8.1630e-01,\n",
       "           -4.2633e-01,  4.1039e-01],\n",
       "          [ 1.3974e-02, -5.6850e-02, -4.0642e-03,  ...,  7.8790e-05,\n",
       "           -1.4510e-02, -4.7457e-02]]]),\n",
       " tensor([[[-1.5213e-01, -9.7455e-01, -5.6574e-01,  ..., -6.0024e-01,\n",
       "            3.0606e-01,  3.6931e-01],\n",
       "          [ 2.7012e-01, -5.4540e-01,  1.6653e-02,  ...,  6.2675e-02,\n",
       "            2.6824e-01,  3.4129e-02],\n",
       "          [-4.1255e-01, -6.4567e-01, -2.0088e-01,  ...,  2.3458e-01,\n",
       "            1.4713e-01,  4.0941e-03],\n",
       "          ...,\n",
       "          [ 1.3217e+00, -6.6628e-02, -8.1355e-01,  ..., -3.9645e-01,\n",
       "           -5.2720e-01,  2.0277e-01],\n",
       "          [-5.6073e-01, -4.5253e-01, -5.8201e-01,  ..., -1.4335e+00,\n",
       "            2.8755e-01,  5.9470e-01],\n",
       "          [ 6.2754e-05, -2.2453e-02, -2.1868e-02,  ..., -1.4897e-02,\n",
       "            2.8922e-03, -4.3436e-02]]]),\n",
       " tensor([[[ 0.0967, -0.8567, -0.5056,  ..., -0.4204,  0.1267,  0.3225],\n",
       "          [-0.0109, -0.2976,  0.2986,  ..., -0.3344,  0.2755, -0.1571],\n",
       "          [-0.6171, -0.4909, -0.1630,  ..., -0.3340,  0.4452, -0.2641],\n",
       "          ...,\n",
       "          [ 0.9738,  0.0119, -0.6562,  ..., -0.3114, -0.1033,  0.1980],\n",
       "          [-0.8265, -0.4334, -0.7586,  ..., -1.2721,  0.0482,  0.3751],\n",
       "          [ 0.0196,  0.0017,  0.0323,  ..., -0.0348, -0.0410, -0.0743]]]),\n",
       " tensor([[[-0.3495, -0.7788, -0.7701,  ..., -0.0715,  0.2350,  0.1686],\n",
       "          [-0.2694, -0.3884, -0.3181,  ..., -0.7334,  0.0196, -0.4148],\n",
       "          [-0.5444, -0.3546, -0.2131,  ..., -0.5306,  0.2799, -0.3587],\n",
       "          ...,\n",
       "          [ 0.6356,  0.0980, -0.2775,  ..., -0.6243, -0.2620,  0.1343],\n",
       "          [-0.4167,  0.1132, -0.4761,  ..., -0.5415, -0.1268,  0.0483],\n",
       "          [-0.0671, -0.0451,  0.0232,  ..., -0.0718, -0.0020, -0.0556]]]),\n",
       " tensor([[[-0.9560, -1.0372, -0.8875,  ...,  0.0905, -0.3867,  0.3258],\n",
       "          [-0.3215, -1.1213, -0.2349,  ..., -0.5940,  0.1851, -0.5076],\n",
       "          [-0.4795, -1.0745, -0.1744,  ..., -0.5026,  0.1293, -0.1963],\n",
       "          ...,\n",
       "          [ 0.2736, -0.4021, -0.1525,  ..., -0.7099, -0.7498, -0.0066],\n",
       "          [-0.0143,  0.0187, -0.0490,  ..., -0.0197, -0.0281,  0.0165],\n",
       "          [-0.5765, -0.5547, -0.2122,  ...,  0.2610, -0.4111, -0.1223]]]),\n",
       " tensor([[[-0.6121, -0.6371, -0.8917,  ...,  0.1026, -0.2241,  0.3330],\n",
       "          [-0.2817, -0.6142, -0.4499,  ..., -0.7273,  0.0304, -0.5106],\n",
       "          [-0.4519, -0.2627, -0.1917,  ..., -0.5405, -0.2146, -0.2140],\n",
       "          ...,\n",
       "          [ 0.3213, -0.2997, -0.0471,  ..., -0.8094, -0.7861, -0.0618],\n",
       "          [ 0.0430,  0.0219, -0.0214,  ...,  0.0196, -0.0353,  0.0072],\n",
       "          [-0.0599, -0.6397, -0.6008,  ...,  0.4218, -0.5170, -0.1616]]]),\n",
       " tensor([[[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
       "          [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
       "          [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
       "          ...,\n",
       "          [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
       "          [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
       "          [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]]]))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "47d83b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 1, 22, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
       "            3.8253e-02,  1.6400e-01],\n",
       "          [ 2.3295e-01,  1.3898e-01,  2.9788e-01,  ..., -6.5465e-02,\n",
       "            8.8849e-01,  5.1089e-01],\n",
       "          [ 2.2572e-01, -7.1647e-01, -7.2547e-01,  ...,  4.8439e-01,\n",
       "            6.0302e-01, -9.5701e-02],\n",
       "          ...,\n",
       "          [-3.7402e-02, -6.1545e-01, -1.4419e+00,  ...,  7.9256e-02,\n",
       "           -8.1097e-02, -3.8018e-01],\n",
       "          [-2.2755e-02,  4.2067e-01, -3.2878e-01,  ...,  4.4641e-01,\n",
       "            5.1775e-01,  5.5010e-01],\n",
       "          [-2.3496e-01,  1.5656e-01, -4.6245e-02,  ..., -4.2065e-01,\n",
       "            3.0737e-01, -2.2883e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.2195e-02,  5.9528e-02, -2.1788e-01,  ...,  2.2799e-01,\n",
       "           -7.1235e-02,  1.4850e-02],\n",
       "          [ 3.8188e-01,  1.4754e-01,  2.4141e-01,  ...,  3.3967e-01,\n",
       "            7.6073e-01,  4.9991e-01],\n",
       "          [ 1.7047e-01, -6.1683e-01, -7.2964e-01,  ...,  8.6309e-01,\n",
       "            6.2739e-01, -3.7271e-01],\n",
       "          ...,\n",
       "          [ 6.9817e-01, -4.5541e-01, -1.7845e+00,  ...,  3.3082e-01,\n",
       "            7.0954e-02, -5.1872e-01],\n",
       "          [-9.0503e-02,  1.8623e-01, -4.4370e-01,  ...,  2.2435e-01,\n",
       "            1.8097e-01,  3.7402e-01],\n",
       "          [-8.2526e-02,  4.6579e-02, -1.5260e-01,  ..., -2.0332e-01,\n",
       "            3.3697e-01, -1.7667e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.5666e-02, -2.0223e-01, -4.1032e-01,  ...,  2.5113e-01,\n",
       "            5.8552e-02, -5.4653e-02],\n",
       "          [ 1.4298e-01,  7.4721e-02,  5.9477e-02,  ..., -2.9141e-01,\n",
       "            1.7326e-01,  4.2652e-01],\n",
       "          [ 5.7522e-01, -1.2385e+00, -5.6504e-01,  ...,  8.9952e-01,\n",
       "            4.6518e-01, -8.0804e-01],\n",
       "          ...,\n",
       "          [ 9.1570e-01, -4.1177e-01, -1.6042e+00,  ...,  1.4539e-01,\n",
       "            1.6985e-01, -2.2298e-01],\n",
       "          [-1.8156e-01,  9.9375e-03,  3.6962e-02,  ..., -2.1779e-01,\n",
       "            4.8058e-02,  3.4768e-01],\n",
       "          [-1.5242e-01, -1.0677e-01, -8.6771e-02,  ..., -1.3689e-01,\n",
       "            2.6327e-01, -3.7541e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-9.5604e-01, -1.0372e+00, -8.8754e-01,  ...,  9.0501e-02,\n",
       "           -3.8670e-01,  3.2580e-01],\n",
       "          [-3.2149e-01, -1.1213e+00, -2.3492e-01,  ..., -5.9404e-01,\n",
       "            1.8506e-01, -5.0758e-01],\n",
       "          [-4.7948e-01, -1.0745e+00, -1.7443e-01,  ..., -5.0257e-01,\n",
       "            1.2932e-01, -1.9628e-01],\n",
       "          ...,\n",
       "          [ 2.7364e-01, -4.0208e-01, -1.5246e-01,  ..., -7.0990e-01,\n",
       "           -7.4977e-01, -6.6032e-03],\n",
       "          [-1.4250e-02,  1.8744e-02, -4.9021e-02,  ..., -1.9688e-02,\n",
       "           -2.8110e-02,  1.6460e-02],\n",
       "          [-5.7646e-01, -5.5467e-01, -2.1221e-01,  ...,  2.6103e-01,\n",
       "           -4.1106e-01, -1.2234e-01]]],\n",
       "\n",
       "\n",
       "        [[[-6.1212e-01, -6.3712e-01, -8.9171e-01,  ...,  1.0260e-01,\n",
       "           -2.2410e-01,  3.3305e-01],\n",
       "          [-2.8169e-01, -6.1423e-01, -4.4993e-01,  ..., -7.2730e-01,\n",
       "            3.0354e-02, -5.1065e-01],\n",
       "          [-4.5192e-01, -2.6275e-01, -1.9171e-01,  ..., -5.4054e-01,\n",
       "           -2.1462e-01, -2.1396e-01],\n",
       "          ...,\n",
       "          [ 3.2134e-01, -2.9974e-01, -4.7084e-02,  ..., -8.0937e-01,\n",
       "           -7.8612e-01, -6.1832e-02],\n",
       "          [ 4.3049e-02,  2.1942e-02, -2.1436e-02,  ...,  1.9552e-02,\n",
       "           -3.5334e-02,  7.2243e-03],\n",
       "          [-5.9901e-02, -6.3968e-01, -6.0083e-01,  ...,  4.2177e-01,\n",
       "           -5.1704e-01, -1.6159e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.9644e-01, -1.8308e-01, -5.2315e-01,  ..., -1.9021e-01,\n",
       "            3.7380e-01,  3.9644e-01],\n",
       "          [-1.3227e-01, -2.7623e-01, -3.4954e-01,  ..., -4.5666e-01,\n",
       "            3.7865e-01, -1.0961e-01],\n",
       "          [-3.6261e-01, -4.0016e-01,  6.7573e-02,  ..., -3.2071e-01,\n",
       "           -2.7090e-01, -3.0043e-01],\n",
       "          ...,\n",
       "          [ 2.9609e-01, -2.8563e-01, -3.8184e-02,  ..., -6.0557e-01,\n",
       "           -5.1633e-01,  2.0051e-01],\n",
       "          [ 4.8782e-01, -9.0928e-02, -2.3581e-01,  ..., -1.7204e-03,\n",
       "           -5.9445e-01, -2.4313e-01],\n",
       "          [-2.5167e-01, -3.5192e-01, -4.6880e-01,  ...,  2.5005e-01,\n",
       "            3.3592e-02, -2.6271e-01]]]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the thing is a tuple, which is kinda useless to us\n",
    "\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "print(token_embeddings.shape)\n",
    "token_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "28d357d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 22, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
       "           3.8253e-02,  1.6400e-01],\n",
       "         [ 2.3295e-01,  1.3898e-01,  2.9788e-01,  ..., -6.5465e-02,\n",
       "           8.8849e-01,  5.1089e-01],\n",
       "         [ 2.2572e-01, -7.1647e-01, -7.2547e-01,  ...,  4.8439e-01,\n",
       "           6.0302e-01, -9.5701e-02],\n",
       "         ...,\n",
       "         [-3.7402e-02, -6.1545e-01, -1.4419e+00,  ...,  7.9256e-02,\n",
       "          -8.1097e-02, -3.8018e-01],\n",
       "         [-2.2755e-02,  4.2067e-01, -3.2878e-01,  ...,  4.4641e-01,\n",
       "           5.1775e-01,  5.5010e-01],\n",
       "         [-2.3496e-01,  1.5656e-01, -4.6245e-02,  ..., -4.2065e-01,\n",
       "           3.0737e-01, -2.2883e-01]],\n",
       "\n",
       "        [[ 5.2195e-02,  5.9528e-02, -2.1788e-01,  ...,  2.2799e-01,\n",
       "          -7.1235e-02,  1.4850e-02],\n",
       "         [ 3.8188e-01,  1.4754e-01,  2.4141e-01,  ...,  3.3967e-01,\n",
       "           7.6073e-01,  4.9991e-01],\n",
       "         [ 1.7047e-01, -6.1683e-01, -7.2964e-01,  ...,  8.6309e-01,\n",
       "           6.2739e-01, -3.7271e-01],\n",
       "         ...,\n",
       "         [ 6.9817e-01, -4.5541e-01, -1.7845e+00,  ...,  3.3082e-01,\n",
       "           7.0954e-02, -5.1872e-01],\n",
       "         [-9.0503e-02,  1.8623e-01, -4.4370e-01,  ...,  2.2435e-01,\n",
       "           1.8097e-01,  3.7402e-01],\n",
       "         [-8.2526e-02,  4.6579e-02, -1.5260e-01,  ..., -2.0332e-01,\n",
       "           3.3697e-01, -1.7667e-01]],\n",
       "\n",
       "        [[-3.5666e-02, -2.0223e-01, -4.1032e-01,  ...,  2.5113e-01,\n",
       "           5.8552e-02, -5.4653e-02],\n",
       "         [ 1.4298e-01,  7.4721e-02,  5.9477e-02,  ..., -2.9141e-01,\n",
       "           1.7326e-01,  4.2652e-01],\n",
       "         [ 5.7522e-01, -1.2385e+00, -5.6504e-01,  ...,  8.9952e-01,\n",
       "           4.6518e-01, -8.0804e-01],\n",
       "         ...,\n",
       "         [ 9.1570e-01, -4.1177e-01, -1.6042e+00,  ...,  1.4539e-01,\n",
       "           1.6985e-01, -2.2298e-01],\n",
       "         [-1.8156e-01,  9.9375e-03,  3.6962e-02,  ..., -2.1779e-01,\n",
       "           4.8058e-02,  3.4768e-01],\n",
       "         [-1.5242e-01, -1.0677e-01, -8.6771e-02,  ..., -1.3689e-01,\n",
       "           2.6327e-01, -3.7541e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-9.5604e-01, -1.0372e+00, -8.8754e-01,  ...,  9.0501e-02,\n",
       "          -3.8670e-01,  3.2580e-01],\n",
       "         [-3.2149e-01, -1.1213e+00, -2.3492e-01,  ..., -5.9404e-01,\n",
       "           1.8506e-01, -5.0758e-01],\n",
       "         [-4.7948e-01, -1.0745e+00, -1.7443e-01,  ..., -5.0257e-01,\n",
       "           1.2932e-01, -1.9628e-01],\n",
       "         ...,\n",
       "         [ 2.7364e-01, -4.0208e-01, -1.5246e-01,  ..., -7.0990e-01,\n",
       "          -7.4977e-01, -6.6032e-03],\n",
       "         [-1.4250e-02,  1.8744e-02, -4.9021e-02,  ..., -1.9688e-02,\n",
       "          -2.8110e-02,  1.6460e-02],\n",
       "         [-5.7646e-01, -5.5467e-01, -2.1221e-01,  ...,  2.6103e-01,\n",
       "          -4.1106e-01, -1.2234e-01]],\n",
       "\n",
       "        [[-6.1212e-01, -6.3712e-01, -8.9171e-01,  ...,  1.0260e-01,\n",
       "          -2.2410e-01,  3.3305e-01],\n",
       "         [-2.8169e-01, -6.1423e-01, -4.4993e-01,  ..., -7.2730e-01,\n",
       "           3.0354e-02, -5.1065e-01],\n",
       "         [-4.5192e-01, -2.6275e-01, -1.9171e-01,  ..., -5.4054e-01,\n",
       "          -2.1462e-01, -2.1396e-01],\n",
       "         ...,\n",
       "         [ 3.2134e-01, -2.9974e-01, -4.7084e-02,  ..., -8.0937e-01,\n",
       "          -7.8612e-01, -6.1832e-02],\n",
       "         [ 4.3049e-02,  2.1942e-02, -2.1436e-02,  ...,  1.9552e-02,\n",
       "          -3.5334e-02,  7.2243e-03],\n",
       "         [-5.9901e-02, -6.3968e-01, -6.0083e-01,  ...,  4.2177e-01,\n",
       "          -5.1704e-01, -1.6159e-01]],\n",
       "\n",
       "        [[-4.9644e-01, -1.8308e-01, -5.2315e-01,  ..., -1.9021e-01,\n",
       "           3.7380e-01,  3.9644e-01],\n",
       "         [-1.3227e-01, -2.7623e-01, -3.4954e-01,  ..., -4.5666e-01,\n",
       "           3.7865e-01, -1.0961e-01],\n",
       "         [-3.6261e-01, -4.0016e-01,  6.7573e-02,  ..., -3.2071e-01,\n",
       "          -2.7090e-01, -3.0043e-01],\n",
       "         ...,\n",
       "         [ 2.9609e-01, -2.8563e-01, -3.8184e-02,  ..., -6.0557e-01,\n",
       "          -5.1633e-01,  2.0051e-01],\n",
       "         [ 4.8782e-01, -9.0928e-02, -2.3581e-01,  ..., -1.7204e-03,\n",
       "          -5.9445e-01, -2.4313e-01],\n",
       "         [-2.5167e-01, -3.5192e-01, -4.6880e-01,  ...,  2.5005e-01,\n",
       "           3.3592e-02, -2.6271e-01]]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sentence Dimension is useless:\n",
    "# We always only pass one dimension, so we might as well drop it\n",
    "\n",
    "# [layer, word, embedding]\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "print(token_embeddings.shape)\n",
    "token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5f1faf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 13, 768])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "token_embeddings.shape # [word, layer, embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "48c13f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for token in token_embeddings:\n",
    "    emb = torch.sum(token[-4:], dim=0)\n",
    "    print(emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "81810be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 768])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings[:,-2:,:].sum(dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e01f5",
   "metadata": {},
   "source": [
    "ok, we created 768dim embeddings for every word in the sentence for the standard Bert Model. Lets check if we can recreate that with the redewiedergabe model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db290ba",
   "metadata": {},
   "source": [
    "# Redewiedergabe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0da9698f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at redewiedergabe/bert-base-historical-german-rw-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"redewiedergabe/bert-base-historical-german-rw-cased\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"redewiedergabe/bert-base-historical-german-rw-cased\",\n",
    "                                             # Whether the model returns all hidden-states.\n",
    "                                             output_hidden_states = True,\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "76de56a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]             3\n",
      "Wenn          2,111\n",
      "es              229\n",
      "dem             128\n",
      "Verfasser    18,241\n",
      "gelungen      9,136\n",
      "ist             127\n",
      ",             2,036\n",
      "ein              39\n",
      "gesichert    10,776\n",
      "##es             16\n",
      "Fundament    17,969\n",
      "und              42\n",
      "die              30\n",
      "ersten          781\n",
      "Pfeil        21,396\n",
      "##er              6\n",
      "für             142\n",
      "die              30\n",
      "Lösung        6,726\n",
      "der              21\n",
      "hier            702\n",
      "zur             252\n",
      "Bearbeitung  16,936\n",
      "gestellten   11,198\n",
      "Aufgabe       4,177\n",
      "zu               81\n",
      "legen         8,482\n",
      ",             2,036\n",
      "so              181\n",
      "ist             127\n",
      "seine           498\n",
      "Absicht      11,589\n",
      "erreicht      3,176\n",
      ";            17,136\n",
      "den              86\n",
      "vollendet    18,814\n",
      "##en              7\n",
      "Bau             788\n",
      "wird            292\n",
      "gab           1,301\n",
      "##e          26,897\n",
      "zu               81\n",
      "legen         8,482\n",
      ",             2,036\n",
      "so              181\n",
      "ist             127\n",
      "seine           498\n",
      "Absicht      11,589\n",
      "erreicht      3,176\n",
      ";            17,136\n",
      "den              86\n",
      "vollendet    18,814\n",
      "##en              7\n",
      "Bau             788\n",
      "wird            292\n",
      "nach            188\n",
      "seiner          471\n",
      "Überzeugung   9,323\n",
      "erst            624\n",
      "eine            155\n",
      "spätere       8,626\n",
      "Generation    8,454\n",
      "errichten     9,725\n",
      "können          718\n",
      ",             2,036\n",
      "wenn            557\n",
      "die              30\n",
      "heutigen      3,601\n",
      "frag         10,160\n",
      "##ment        3,758\n",
      "##arischen    6,196\n",
      "und              42\n",
      "un              174\n",
      "##fert        1,816\n",
      "##igen          219\n",
      "Hyp           9,454\n",
      "##othe        3,005\n",
      "##sen           138\n",
      "über            204\n",
      "die              30\n",
      "Beziehungen   7,171\n",
      "zwischen        597\n",
      "den              86\n",
      "ant           3,631\n",
      "##hrop       11,724\n",
      "##ologischen  4,508\n",
      "Eigent        3,652\n",
      "##üm          3,877\n",
      "##lichkeiten  5,354\n",
      "der              21\n",
      "Völker        9,102\n",
      "und              42\n",
      "ihrer           811\n",
      "geschicht    23,541\n",
      "##lichen        248\n",
      "Bedeutung     2,322\n",
      "zu               81\n",
      "wissenschaft  3,848\n",
      "##lich           68\n",
      "gesichert    10,776\n",
      "##en              7\n",
      "Ergebnissen  18,900\n",
      "geworden      2,750\n",
      "sch             205\n",
      "##icht           90\n",
      "##lichen        248\n",
      "Bedeutung     2,322\n",
      "zu               81\n",
      "wissenschaft  3,848\n",
      "##lich           68\n",
      "gesichert    10,776\n",
      "##en              7\n",
      "Ergebnissen  18,900\n",
      "geworden      2,750\n",
      "sein            167\n",
      "werden          266\n",
      ".             4,813\n",
      "[SEP]             4\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \" Wenn es dem Verfasser gelungen ist, ein gesichertes Fundament und die ersten Pfeiler für die Lösung der hier zur Bearbeitung gestellten Aufgabe zu legen, so ist seine Absicht erreicht; den vollendeten Bau wird gabe zu legen, so ist seine Absicht erreicht; den vollendeten Bau wird nach seiner Überzeugung erst eine spätere Generation errichten können, wenn die heutigen fragmentarischen und unfertigen Hypothesen über die Beziehungen zwischen den anthropologischen Eigentümlichkeiten der Völker und ihrer geschichtlichen Bedeutung zu wissenschaftlich gesicherten Ergebnissen geworden schichtlichen Bedeutung zu wissenschaftlich gesicherten Ergebnissen geworden sein werden.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "65ae762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "af6247a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put model in feed forward mode\n",
    "model.eval();\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    \n",
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "15aafc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacked:   torch.Size([13, 1, 119, 768])\n",
      "squeezed:  torch.Size([13, 119, 768])\n",
      "permuted:  torch.Size([119, 13, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0926, -0.8170,  0.0326,  ..., -0.1046,  0.0433, -0.5012],\n",
       "         [ 0.0194, -0.3121,  0.1395,  ..., -0.1639, -0.0460, -0.2086],\n",
       "         [-0.0557, -0.3616,  0.0786,  ..., -0.2102, -0.0840, -0.2077],\n",
       "         ...,\n",
       "         [-0.7274,  0.5587,  0.2301,  ...,  0.4055, -0.2140, -0.0942],\n",
       "         [-0.6234,  0.5077, -0.0440,  ...,  0.6145, -0.1996, -0.3484],\n",
       "         [-0.1697,  0.0913, -0.0151,  ...,  0.6369, -0.2115, -0.4350]],\n",
       "\n",
       "        [[-2.2877,  1.4302, -1.3100,  ..., -0.4781, -0.9969, -0.7987],\n",
       "         [-2.1292,  1.1550, -1.0924,  ..., -0.3603, -0.5673, -0.0540],\n",
       "         [-2.1085,  0.8840, -1.1395,  ..., -0.0757, -0.6780, -0.1082],\n",
       "         ...,\n",
       "         [-0.9193,  0.7059, -1.0839,  ...,  0.3777,  0.3921, -0.1505],\n",
       "         [-1.1245,  0.5554, -1.4194,  ...,  0.0392,  0.5612, -0.1387],\n",
       "         [-1.0576, -0.2039, -1.3393,  ...,  0.3028,  0.4234, -0.2648]],\n",
       "\n",
       "        [[-1.2646,  1.0128,  1.0663,  ..., -0.1060,  0.7109, -1.2380],\n",
       "         [-1.3131,  1.3647,  0.6510,  ..., -0.6550,  0.7616, -0.5961],\n",
       "         [-1.5793,  1.0468,  0.3072,  ..., -0.8775,  0.5291, -0.8498],\n",
       "         ...,\n",
       "         [-0.9843, -0.7726, -0.4404,  ..., -0.4142,  1.1900, -1.0759],\n",
       "         [-0.6330, -0.4602, -1.2816,  ..., -0.2730,  2.0235, -0.8834],\n",
       "         [-0.7213, -0.5577, -1.1670,  ..., -0.8318,  1.0346, -0.5013]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4365, -1.6605,  0.3484,  ...,  0.0816, -0.5362, -1.5063],\n",
       "         [-0.5188, -0.4603,  0.8342,  ...,  0.3000, -0.6034, -1.6634],\n",
       "         [-0.8546,  0.2579,  1.1354,  ..., -0.3910, -0.8981, -1.9412],\n",
       "         ...,\n",
       "         [-0.0263, -0.3302, -0.5096,  ..., -0.2703, -0.5651, -2.8021],\n",
       "         [ 0.8235, -0.1394, -1.3487,  ...,  0.3354, -0.5329, -2.8850],\n",
       "         [ 0.5115, -0.6822, -1.1209,  ...,  0.5445, -0.9732, -1.4149]],\n",
       "\n",
       "        [[ 0.3282, -1.8211, -0.6508,  ...,  1.1668, -1.0766, -1.0900],\n",
       "         [ 0.0963, -0.6089, -0.3683,  ...,  0.9068, -0.5438, -0.9124],\n",
       "         [-0.2319, -0.5746, -0.3480,  ...,  0.6087, -0.2758, -0.9122],\n",
       "         ...,\n",
       "         [-0.8759, -0.3220, -0.5071,  ...,  0.3613, -0.4020, -0.7852],\n",
       "         [-1.3885,  0.1206, -1.0244,  ...,  0.8350, -0.4496, -1.6626],\n",
       "         [-0.7617, -0.3751, -0.3963,  ...,  0.6455, -0.3997, -0.9600]],\n",
       "\n",
       "        [[ 0.6106, -1.8794,  0.7351,  ...,  0.8468,  0.7038, -0.2820],\n",
       "         [ 0.1030, -0.9063,  0.7527,  ...,  0.9275,  0.5036, -0.1813],\n",
       "         [-0.1474, -0.6228,  0.6066,  ...,  0.8207,  0.7256, -0.0601],\n",
       "         ...,\n",
       "         [-0.0077,  0.0711,  0.0538,  ...,  0.0125,  0.0909,  0.0424],\n",
       "         [ 0.0189,  0.0290,  0.0153,  ...,  0.0179,  0.0628,  0.0250],\n",
       "         [ 0.0909, -0.2530, -0.8706,  ...,  0.5650, -0.0594, -1.6540]]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "# [layer, sentence, word, embedding]\n",
    "token_embeddings = torch.stack(hidden_states, dim=0); print(\"stacked:  \", token_embeddings.shape)\n",
    "\n",
    "# [layer, word, embedding]\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1); print(\"squeezed: \", token_embeddings.shape)\n",
    "\n",
    "# [word, layer, embedding]\n",
    "token_embeddings = token_embeddings.permute(1,0,2); print(\"permuted: \", token_embeddings.shape) \n",
    "token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4f3725ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([119, 768])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings[:,-2:,:].sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cabb6c5",
   "metadata": {},
   "source": [
    "# turn it into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773547ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "def embeddings(sentence, last_layers = 2):\n",
    "    \"\"\"\n",
    "    Create embeddings for a sentence using the redewiedergabe model.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    model_name = \"redewiedergabe/bert-base-historical-german-rw-cased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name,\n",
    "                                                 output_hidden_states = True)\n",
    "    \n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    # only a single segment for a single sentence\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # put model in feed forward mode\n",
    "    model.eval();\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        \n",
    "    hidden_states = outputs.hidden_states\n",
    "    # [layer, sentence, word, embedding]\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)#; print(\"stacked:  \", token_embeddings.shape)\n",
    "    # [layer, word, embedding]\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)#; print(\"squeezed: \", token_embeddings.shape)\n",
    "    # [word, layer, embedding]\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)#; print(\"permuted: \", token_embeddings.shape) \n",
    "    \n",
    "    return token_embeddings[:,-last_layers:,:].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85aed55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e36b316",
   "metadata": {},
   "source": [
    "# load the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db949e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Textstelle</th>\n",
       "      <th>Metapher?</th>\n",
       "      <th>Fokus</th>\n",
       "      <th>Rahmen</th>\n",
       "      <th>Stärkegrad (A, B, C)</th>\n",
       "      <th>Begründung/Kommentar</th>\n",
       "      <th>Annotator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bei Beobachtung solchen moralischen Wertes ka...</td>\n",
       "      <td>Metapher</td>\n",
       "      <td>das Horoskop stellen</td>\n",
       "      <td>einer Nation</td>\n",
       "      <td>B</td>\n",
       "      <td>Horoskop stellen - bezogen auf Nationen ist da...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Die Zellen verschmelzen miteinander.</td>\n",
       "      <td>Metaphernkandidat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fachausdruck</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Diese wolle die bittere Auslese, ohne die auc...</td>\n",
       "      <td>Metapher</td>\n",
       "      <td>bittere</td>\n",
       "      <td>Auslese</td>\n",
       "      <td>A</td>\n",
       "      <td>Unauffällig, aber doch metaphorisch: Dass eine...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wenn es dem Verfasser gelungen ist, ein gesic...</td>\n",
       "      <td>Metapher</td>\n",
       "      <td>ein gesichertes Fundament und die ersten Pfeil...</td>\n",
       "      <td>die Lösung der hier zur Bearbeitung gestellten...</td>\n",
       "      <td>A</td>\n",
       "      <td>Bruch, Fokus nicht ohne Bedeutungsverlust erse...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In Californien ist ebenso die früher dort hei...</td>\n",
       "      <td>Metaphernkandidat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kein Bruch, nur auffälliger Ausdruck, keine Be...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Textstelle          Metapher?  \\\n",
       "0   Bei Beobachtung solchen moralischen Wertes ka...           Metapher   \n",
       "1               Die Zellen verschmelzen miteinander.  Metaphernkandidat   \n",
       "2   Diese wolle die bittere Auslese, ohne die auc...           Metapher   \n",
       "3   Wenn es dem Verfasser gelungen ist, ein gesic...           Metapher   \n",
       "4   In Californien ist ebenso die früher dort hei...  Metaphernkandidat   \n",
       "\n",
       "                                               Fokus  \\\n",
       "0                               das Horoskop stellen   \n",
       "1                                                NaN   \n",
       "2                                            bittere   \n",
       "3  ein gesichertes Fundament und die ersten Pfeil...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              Rahmen Stärkegrad (A, B, C)  \\\n",
       "0                                       einer Nation                    B   \n",
       "1                                                NaN                  NaN   \n",
       "2                                            Auslese                    A   \n",
       "3  die Lösung der hier zur Bearbeitung gestellten...                    A   \n",
       "4                                                NaN                  NaN   \n",
       "\n",
       "                                Begründung/Kommentar Annotator  \n",
       "0  Horoskop stellen - bezogen auf Nationen ist da...         B  \n",
       "1                                       Fachausdruck         B  \n",
       "2  Unauffällig, aber doch metaphorisch: Dass eine...         B  \n",
       "3  Bruch, Fokus nicht ohne Bedeutungsverlust erse...         B  \n",
       "4  kein Bruch, nur auffälliger Ausdruck, keine Be...         B  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/fynn/Uni/DL4NLP/data/Annotationen-Stufe-2.txt\", index_col=0)\n",
    "df.drop([\"Seite\", \"Unnamed: 2\",\n",
    "         \"Semantikerweiterung?\",\n",
    "         \"Unersetzlich?\",\n",
    "         \"Unersetzlich?\",\n",
    "         \"sprachlich irregulär?\",\n",
    "         \"pointiert?\"], axis = 1, inplace = True) # 2x Floskel\n",
    "\n",
    "df = df[~df.Textstelle.isnull()]\n",
    "df\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472aa66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2566 entries, 0 to 2583\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   Textstelle            2566 non-null   object\n",
      " 1   Metapher?             2566 non-null   object\n",
      " 2   Fokus                 768 non-null    object\n",
      " 3   Rahmen                698 non-null    object\n",
      " 4   Stärkegrad (A, B, C)  651 non-null    object\n",
      " 5   Begründung/Kommentar  833 non-null    object\n",
      " 6   Annotator             2566 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 160.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed1b9614",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B', nan, 'A', 'C', 'A, B', 'B. B', 'A, A', 'B oder C', 'A ',\n",
       "       'A oder B', 'A oder B ', 'C; B ', 'B oder C ', 'B ', 'B, B',\n",
       "       'B; B', 'A oder B; B', 'A; B oder C', 'B oder C; B oder C'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Stärkegrad (A, B, C)\"].unique() # potentielles label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b49dfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"A\"] = [\"A\" in i for i in df[\"Stärkegrad (A, B, C)\"].fillna(\"\")]\n",
    "df[\"B\"] = [\"B\" in i for i in df[\"Stärkegrad (A, B, C)\"].fillna(\"\")]\n",
    "df[\"C\"] = [\"C\" in i for i in df[\"Stärkegrad (A, B, C)\"].fillna(\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "477cc8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Metapher', 'Metaphernkandidat', 'Grenzfall', 'Unklar'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Metapher?\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6801efa2",
   "metadata": {},
   "source": [
    "# Use model on a phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b0bb39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Wenn es dem Verfasser gelungen ist, ein gesichertes Fundament und die ersten Pfeiler für die Lösung der hier zur Bearbeitung gestellten Aufgabe zu legen, so ist seine Absicht erreicht; den vollendeten Bau wird gabe zu legen, so ist seine Absicht erreicht; den vollendeten Bau wird nach seiner Überzeugung erst eine spätere Generation errichten können, wenn die heutigen fragmentarischen und unfertigen Hypothesen über die Beziehungen zwischen den anthropologischen Eigentümlichkeiten der Völker und ihrer geschichtlichen Bedeutung zu wissenschaftlich gesicherten Ergebnissen geworden schichtlichen Bedeutung zu wissenschaftlich gesicherten Ergebnissen geworden sein werden.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df.loc[3, \"Textstelle\"]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea3c3f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wenn',\n",
       " 'es',\n",
       " 'dem',\n",
       " 'Verfasser',\n",
       " 'gelungen',\n",
       " 'ist,',\n",
       " 'ein',\n",
       " 'gesichertes',\n",
       " 'Fundament']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(\" \")[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "19996540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 19181,  2078,   102,     0,     0,     0,     0],\n",
       "        [  101,  9686,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 17183,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  2310, 12881, 27241,  2099,   102,     0,     0],\n",
       "        [  101, 21500, 23239,   102,     0,     0,     0,     0],\n",
       "        [  101, 21541,  1010,   102,     0,     0,     0,     0],\n",
       "        [  101, 16417,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 16216, 19570,  5886,  4570,   102,     0,     0],\n",
       "        [  101,  4636, 24996,   102,     0,     0,     0,     0],\n",
       "        [  101,  6151,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  3280,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  9413, 16173,   102,     0,     0,     0,     0],\n",
       "        [  101,  1052,  7959,  9463,  2099,   102,     0,     0],\n",
       "        [  101,  6519,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  3280,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  3050,  5575,   102,     0,     0,     0,     0],\n",
       "        [  101,  4315,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  7632,  2121,   102,     0,     0,     0,     0],\n",
       "        [  101, 17924,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  4562, 19205, 21847,   102,     0,     0,     0],\n",
       "        [  101, 16216, 13473,  3363,  6528,   102,     0,     0],\n",
       "        [  101, 21200,  3654,  4783,   102,     0,     0,     0],\n",
       "        [  101, 16950,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  4190,  2368,  1010,   102,     0,     0,     0],\n",
       "        [  101,  2061,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 21541,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 16470,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 14689,  7033,  2102,   102,     0,     0,     0],\n",
       "        [  101,  9413,  2890,  7033,  2102,  1025,   102,     0],\n",
       "        [  101,  7939,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  5285,  7770,  3207,  6528,   102,     0,     0],\n",
       "        [  101,  8670,  2226,   102,     0,     0,     0,     0],\n",
       "        [  101, 15536,  4103,   102,     0,     0,     0,     0],\n",
       "        [  101, 12900,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 16950,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  4190,  2368,  1010,   102,     0,     0,     0],\n",
       "        [  101,  2061,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 21541,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 16470,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 14689,  7033,  2102,   102,     0,     0,     0],\n",
       "        [  101,  9413,  2890,  7033,  2102,  1025,   102,     0],\n",
       "        [  101,  7939,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  5285,  7770,  3207,  6528,   102,     0,     0],\n",
       "        [  101,  8670,  2226,   102,     0,     0,     0,     0],\n",
       "        [  101, 15536,  4103,   102,     0,     0,     0,     0],\n",
       "        [  101,  6583,  2818,   102,     0,     0,     0,     0],\n",
       "        [  101, 16470,  2099,   102,     0,     0,     0,     0],\n",
       "        [  101, 19169,  4371, 15916,  5575,   102,     0,     0],\n",
       "        [  101,  9413,  3367,   102,     0,     0,     0,     0],\n",
       "        [  101, 27665,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 14690,  7869,   102,     0,     0,     0,     0],\n",
       "        [  101,  4245,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  9413, 13149,  6528,   102,     0,     0,     0],\n",
       "        [  101, 12849, 10087,  2078,  1010,   102,     0,     0],\n",
       "        [  101, 19181,  2078,   102,     0,     0,     0,     0],\n",
       "        [  101,  3280,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  2002, 21823,  6914,   102,     0,     0,     0],\n",
       "        [  101, 15778, 23061,  8661,   102,     0,     0,     0],\n",
       "        [  101,  6151,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  4895,  7512,  3775,  6914,   102,     0,     0],\n",
       "        [  101,  1044, 22571, 14573,  6810,  2078,   102,     0],\n",
       "        [  101, 19169,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  3280,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  2022, 14272, 17157,  6914,   102,     0,     0],\n",
       "        [  101,  1062,  9148, 23796,   102,     0,     0,     0],\n",
       "        [  101,  7939,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 14405,  8093,  7361, 12898, 17701,  8661,   102],\n",
       "        [  101,  1041, 29206, 11667, 18337, 29501,  6528,   102],\n",
       "        [  101,  4315,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  5285,  5484,   102,     0,     0,     0,     0],\n",
       "        [  101,  6151,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  1045, 17875,   102,     0,     0,     0,     0],\n",
       "        [  101, 16216, 11624,  7033, 19646, 17322,  2078,   102],\n",
       "        [  101,  2793, 13765, 21847,   102,     0,     0,     0],\n",
       "        [  101, 16950,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 15536, 14416, 26527, 18337,   102,     0,     0],\n",
       "        [  101, 16216, 19570,  5886,  6528,   102,     0,     0],\n",
       "        [  101,  9413,  3351, 24700, 23491,  2078,   102,     0],\n",
       "        [  101, 16216, 18351,  2368,   102,     0,     0,     0],\n",
       "        [  101,  8040, 16066, 11039, 27412,  2078,   102,     0],\n",
       "        [  101,  2793, 13765, 21847,   102,     0,     0,     0],\n",
       "        [  101, 16950,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 15536, 14416, 26527, 18337,   102,     0,     0],\n",
       "        [  101, 16216, 19570,  5886,  6528,   102,     0,     0],\n",
       "        [  101,  9413,  3351, 24700, 23491,  2078,   102,     0],\n",
       "        [  101, 16216, 18351,  2368,   102,     0,     0,     0],\n",
       "        [  101,  7367,  2378,   102,     0,     0,     0,     0],\n",
       "        [  101,  2057, 18246,  1012,   102,     0,     0,     0]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text.split(\" \")[1:],\n",
    "          return_tensors=\"pt\",\n",
    "          padding=True\n",
    "         )[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2c757d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([88, 8, 30522])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tokenizer(text.split(\" \")[1:],\n",
    "          return_tensors=\"pt\",\n",
    "          padding=True\n",
    "         )\n",
    "     )[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d681ff2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 19181,  2078,  9686, 17183,  2310, 12881, 27241,  2099, 21500,\n",
       "         23239, 21541,  1010, 16417, 16216, 19570,  5886,  4570,  4636, 24996,\n",
       "          6151,  3280,  9413, 16173,  1052,  7959,  9463,  2099,  6519,  3280,\n",
       "          3050,  5575,  4315,  7632,  2121, 17924,  4562, 19205, 21847, 16216,\n",
       "         13473,  3363,  6528, 21200,  3654,  4783, 16950,  4190,  2368,  1010,\n",
       "          2061, 21541, 16470, 14689,  7033,  2102,  9413,  2890,  7033,  2102,\n",
       "          1025,  7939,  5285,  7770,  3207,  6528,  8670,  2226, 15536,  4103,\n",
       "         12900, 16950,  4190,  2368,  1010,  2061, 21541, 16470, 14689,  7033,\n",
       "          2102,  9413,  2890,  7033,  2102,  1025,  7939,  5285,  7770,  3207,\n",
       "          6528,  8670,  2226, 15536,  4103,  6583,  2818, 16470,  2099, 19169,\n",
       "          4371, 15916,  5575,  9413,  3367, 27665, 14690,  7869,  4245,  9413,\n",
       "         13149,  6528, 12849, 10087,  2078,  1010, 19181,  2078,  3280,  2002,\n",
       "         21823,  6914, 15778, 23061,  8661,  6151,  4895,  7512,  3775,  6914,\n",
       "          1044, 22571, 14573,  6810,  2078, 19169,  3280,  2022, 14272, 17157,\n",
       "          6914,  1062,  9148, 23796,  7939, 14405,  8093,  7361, 12898, 17701,\n",
       "          8661,  1041, 29206, 11667, 18337, 29501,  6528,  4315,  5285,  5484,\n",
       "          6151,  1045, 17875, 16216, 11624,  7033, 19646, 17322,  2078,  2793,\n",
       "         13765, 21847, 16950, 15536, 14416, 26527, 18337, 16216, 19570,  5886,\n",
       "          6528,  9413,  3351, 24700, 23491,  2078, 16216, 18351,  2368,  8040,\n",
       "         16066, 11039, 27412,  2078,  2793, 13765, 21847, 16950, 15536, 14416,\n",
       "         26527, 18337, 16216, 19570,  5886,  6528,  9413,  3351, 24700, 23491,\n",
       "          2078, 16216, 18351,  2368,  7367,  2378,  2057, 18246,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(text,\n",
    "                   return_tensors=\"pt\"\n",
    "                  )\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fbae3337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 19181,  2078,  9686, 17183,  2310, 12881, 27241,  2099, 21500,\n",
       "         23239, 21541,  1010, 16417, 16216, 19570,  5886,  4570,  4636, 24996,\n",
       "          6151,  3280,  9413, 16173,  1052,  7959,  9463,  2099,  6519,  3280,\n",
       "          3050,  5575,  4315,  7632,  2121, 17924,  4562, 19205, 21847, 16216,\n",
       "         13473,  3363,  6528, 21200,  3654,  4783, 16950,  4190,  2368,  1010,\n",
       "          2061, 21541, 16470, 14689,  7033,  2102,  9413,  2890,  7033,  2102,\n",
       "          1025,  7939,  5285,  7770,  3207,  6528,  8670,  2226, 15536,  4103,\n",
       "         12900, 16950,  4190,  2368,  1010,  2061, 21541, 16470, 14689,  7033,\n",
       "          2102,  9413,  2890,  7033,  2102,  1025,  7939,  5285,  7770,  3207,\n",
       "          6528,  8670,  2226, 15536,  4103,  6583,  2818, 16470,  2099, 19169,\n",
       "          4371, 15916,  5575,  9413,  3367, 27665, 14690,  7869,  4245,  9413,\n",
       "         13149,  6528, 12849, 10087,  2078,  1010, 19181,  2078,  3280,  2002,\n",
       "         21823,  6914, 15778, 23061,  8661,  6151,  4895,  7512,  3775,  6914,\n",
       "          1044, 22571, 14573,  6810,  2078, 19169,  3280,  2022, 14272, 17157,\n",
       "          6914,  1062,  9148, 23796,  7939, 14405,  8093,  7361, 12898, 17701,\n",
       "          8661,  1041, 29206, 11667, 18337, 29501,  6528,  4315,  5285,  5484,\n",
       "          6151,  1045, 17875, 16216, 11624,  7033, 19646, 17322,  2078,  2793,\n",
       "         13765, 21847, 16950, 15536, 14416, 26527, 18337, 16216, 19570,  5886,\n",
       "          6528,  9413,  3351, 24700, 23491,  2078, 16216, 18351,  2368,  8040,\n",
       "         16066, 11039, 27412,  2078,  2793, 13765, 21847, 16950, 15536, 14416,\n",
       "         26527, 18337, 16216, 19570,  5886,  6528,  9413,  3351, 24700, 23491,\n",
       "          2078, 16216, 18351,  2368,  7367,  2378,  2057, 18246,  1012,   102]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5b92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8aa94d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (651) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) )[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextstelle\u001b[39m\u001b[38;5;124m\"\u001b[39m] ])\n\u001b[1;32m      2\u001b[0m X\n",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextstelle\u001b[39m\u001b[38;5;124m\"\u001b[39m] ])\n\u001b[1;32m      2\u001b[0m X\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1350\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1348\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1350\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1364\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1365\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1010\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1010\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1018\u001b[0m     embedding_output,\n\u001b[1;32m   1019\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1028\u001b[0m )\n\u001b[1;32m   1029\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:241\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    240\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[0;32m--> 241\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m    242\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n\u001b[1;32m    243\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (651) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "X = np.array([model(**tokenizer(text, return_tensors=\"pt\") )[0].mean(axis=1).detach().numpy().reshape(-1) for text in df[\"Textstelle\"] ])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83303718",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Metapher?\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

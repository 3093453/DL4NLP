{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe6bad2",
   "metadata": {},
   "source": [
    "in this notebook we try to recreate the results of the previous group for only the redewiedergabe model without oversampling by translation and without smote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b37f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ast\n",
    "import copy\n",
    "#import csv\n",
    "#import glob\n",
    "#import json\n",
    "import random\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "import deepl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer, BertForSequenceClassification,\n",
    "                          BertModel, Trainer, TrainingArguments)\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.bert.modeling_bert import *\n",
    "from transformers.utils.dummy_tf_objects import TFDPRQuestionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b51752",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.getcwd()\n",
    "DATA_PATH = '/data'\n",
    "MODEL_PATH = '/model'\n",
    "RESULTS_PATH = '/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0326a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this parameter is set on true, oversampling will be performed on the dataframe\n",
    "oversample_dataframe = False\n",
    "\n",
    "# For the use of the Deepl API (is used to oversample the data set), a Deepl Authentication Key is needed\n",
    "# To obtain such a key, a free trial can be started on the following page: https://www.deepl.com/docs-api/\n",
    "# When 500.000 characters have been translated, a credit card is needed to start a real abonnement and to access the API\n",
    "deepl_auth_key = 'DEEPL_AUTH_KEY'\n",
    "\n",
    "# This parameter specifies the number of iterations after which the intermediate result of the dataframe should be stored repeatedly\n",
    "backup_oversampled_dataframe_after_rows = 40\n",
    "\n",
    "def set_default_staerkegrad_df(df):\n",
    "  \"\"\" \n",
    "  set_default_staerkegrad accepts a dataframe as input. Checks if there is a column named \"Staerkegrad\"\n",
    "  and if there is inserts into empty fields in that column the mostly used value from the column.\n",
    "\n",
    "  :param df: a pandas dataframe\n",
    "  :return: a pandas dataframe, where for every undefined entry in the column \"Staerkgegrad\" the most common value from all rows is set. If there is no such column in the input dataframe, the input dataframe is returned.  \n",
    "  \"\"\"\n",
    "  if ('Stärkegrad (A, B, C)' not in df.columns):\n",
    "    print('[set_default_staerkegrad_df]: Given dataframe does not consist of a column \"Staerkegrad\"!')\n",
    "    return df\n",
    "  else:\n",
    "    most_used_staerkegrad = df['Stärkegrad (A, B, C)'].value_counts().index[0]\n",
    "    df.fillna(value={'Stärkegrad (A, B, C)': most_used_staerkegrad})\n",
    "    return df\n",
    "\n",
    "def translate(text, target_language):\n",
    "  \"\"\"\n",
    "  translate translates the input text into the target language.\n",
    "\n",
    "  :param text: the text to be translated\n",
    "  :param target_language: the deepl target language expression, examples are 'DE' or 'EN-US'\n",
    "  :return: a string, the translation of :param text into the :param target_language\n",
    "  \"\"\"\n",
    "  translator = deepl.Translator(deepl_auth_key) \n",
    "  result = translator.translate_text(text, target_lang=target_language) \n",
    "  translated_text = result.text\n",
    "  return translated_text\n",
    "\n",
    "def translate_into_english_and_back(text):\n",
    "  \"\"\"\n",
    "  translate_into_english_and_back translates the input text into english and then into German.\n",
    "\n",
    "  :param text: the text to be translated\n",
    "  :return: a string. Returned is the result from translating :param text into englisch and after that into German.\n",
    "  \"\"\"\n",
    "  translator = deepl.Translator(deepl_auth_key) \n",
    "  result_eng = translator.translate_text(text, target_lang='EN-US')\n",
    "  result_ger = translator.translate_text(result_eng.text, target_lang='DE')\n",
    "  return result_ger.text\n",
    "\n",
    "def translate_into_target_language_and_back(text, target_language):\n",
    "  \"\"\"\n",
    "  translate_into_target_language_and_back translates the input text into the given target_language and then into German.\n",
    "\n",
    "  :param text: the text to be translated\n",
    "  :param target_language: the language\n",
    "  :return: a string. Returned is the result from translating :param text into :param target_language and then into German. \n",
    "  \"\"\"\n",
    "  translator = deepl.Translator(deepl_auth_key) \n",
    "  result_eng = translator.translate_text(text, target_lang=target_language)\n",
    "  result_ger = translator.translate_text(result_eng.text, target_lang='DE')\n",
    "  return result_ger.text\n",
    "\n",
    "def oversample_dataframe(df):\n",
    "  \"\"\"\n",
    "  Accepts a dataframe and returns the dataframe with oversampled data. The function was written for the known dataset of the Goldstandard. \n",
    "  In detail, every Textstelle (from metaphors only) from the input dataframe is taken, translated into four languages (english, spanish, czech and polish) and back into German.\n",
    "  By this way, for each Textstelle from :param df, four new texts are generated and added to the output dataframe. \n",
    "\n",
    "  :param df: a pandas dataframe. \n",
    "  :return: a pandas dataframe. In the output dataframe, four columns have been added, in which the different back and forth translated German texts are. \n",
    "  \"\"\"\n",
    "  # Before the oversampling, the counts of unique rows in the input df and of rows which are metaphors are printed\n",
    "  print('Ausprägungen und Anzahl Werte für gold_standard_df vor Oversampling:', df['Metapher?'].value_counts())\n",
    "  print('Metaphern im gold_standard_df vor Oversampling', df['Metapher?'].value_counts().Metapher)\n",
    "\n",
    "  # To backup interim results, this counter is initialized\n",
    "  count = 1\n",
    "\n",
    "  # To keep count of successfully added translated metaphor texts, the following counter is initialized\n",
    "  count_successfully_added_metaphor_texts = 0\n",
    "\n",
    "  # Split the input dataframe into two dataframes, one containing only metaphors and one with only not metaphors\n",
    "  only_metaphor_df = df[(df['Metapher?'] == 'Metapher')]\n",
    "  no_metaphors_df = df[(df['Metapher?'] != 'Metapher')]\n",
    "\n",
    "  # To the dataframe containing only metaphors, add four columns where the newly generated texts can be inserted to\n",
    "  only_metaphor_df['Synonym (aus Englischem)'] = \"\"\n",
    "  only_metaphor_df['Synonym (aus Spanischem)'] = \"\"\n",
    "  only_metaphor_df['Synonym (aus Tchechischem)'] = \"\"\n",
    "  only_metaphor_df['Synonym (aus Polnischem)'] = \"\"\n",
    "  \n",
    "  # Loop over all rows in the dataframe containing only metaphors and translate the text back and forth and insert the German result in the correct dataframe cell\n",
    "  for index, row in only_metaphor_df.iterrows():\n",
    "    text = row['Textstelle']\n",
    "    row['Synonym (aus Englischem)'] = translate_into_target_language_and_back(text, 'EN-US')\n",
    "    row['Synonym (aus Spanischem)'] = translate_into_target_language_and_back(text, 'ES')\n",
    "    row['Synonym (aus Tchechischem)'] = translate_into_target_language_and_back(text, 'CS')\n",
    "    row['Synonym (aus Polnischem)'] = translate_into_target_language_and_back(text, 'PL')\n",
    "    count_successfully_added_metaphor_texts += 4\n",
    "    print('working, count:', count)\n",
    "\n",
    "    # Backup interim results of the dataframe as csv files every 'backup_oversampled_dataframe_after_rows' iterations\n",
    "    if (count % backup_oversampled_dataframe_after_rows == 0): \n",
    "      only_metaphor_df.to_csv('only_metaphor_df_four_languages_backup_iteration_' + str(count) + '.csv', index=False)\n",
    "\n",
    "    # Increase counter\n",
    "    count+=1\n",
    "\n",
    "  # After the for loop, the two dataframes only metaphor and not metaphors need to get concatenated again\n",
    "  oversampled_data_df = only_metaphor_df.append(no_metaphors_df)\n",
    "\n",
    "  # After the oversampling, print the counts of unique rows in the oversampled dataframe and of rows which are metaphors again to get an overview on the results of the data augmentation \n",
    "  print('Ausprägungen und Anzahl Werte für gold_standard_df nach Oversampling:', oversampled_data_df['Metapher?'].value_counts())\n",
    "  print('Metaphern im gold_standard_df nach Oversampling', oversampled_data_df['Metapher?'].value_counts().Metapher + count_successfully_added_metaphor_texts)\n",
    "\n",
    "  return oversampled_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46436df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_settings(path, annotators, epochs, folds, model_type, oversampling, smote, comment):\n",
    "  \"\"\"\n",
    "  Saves the given settings to a text file at a given path\n",
    "  :param path: the path to output the text file to\n",
    "  :param annotators: The annotators to save \n",
    "  :param epochs: The number of epochs to save \n",
    "  :param folds: The number of folds to save \n",
    "  :param model_type: The model type to save \n",
    "  :param oversampling: The boolean if oversampling is activated \n",
    "  :param smote: The K for SMOTE to save \n",
    "  :param comment: A free comment to save in the text file\n",
    "  \"\"\"  \n",
    "  \n",
    "  lines = []\n",
    "  lines.append('Annotators: ' + ', '.join(annotators))\n",
    "  lines.append('Epochs: ' + str(epochs))\n",
    "  lines.append('Folds: ' + str(folds))\n",
    "  lines.append('Model Type: ' + model_type)\n",
    "  lines.append('Oversampling: ' + (\"On\" if oversampling else \"Off\"))\n",
    "  lines.append('SMOTE: ' + ((\"On (K=\"+str(smote)+\")\") if smote>0 else \"Off\"))\n",
    "  lines.append('Comment: ' + comment)\n",
    "\n",
    "  with open(path+'/settings.txt', 'w') as f: \n",
    "    f.write('\\n'.join(lines))\n",
    "\n",
    "  print(\"Settings saved to\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f75c3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading annotated data for individual annotators\n",
    "raw_df = pd.read_csv(ROOT_PATH + DATA_PATH + '/Annotationen-Stufe-2.txt', index_col=0)\n",
    "# Drop rows where Textstelle is NaN\n",
    "index_names = raw_df[raw_df['Textstelle'].isnull()].index\n",
    "raw_df.drop(index_names, inplace=True)\n",
    "# setting the Stärkegrad if not present\n",
    "raw_df = set_default_staerkegrad_df(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c9043e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seite</th>\n",
       "      <th>Textstelle</th>\n",
       "      <th>Metapher?</th>\n",
       "      <th>Fokus</th>\n",
       "      <th>Rahmen</th>\n",
       "      <th>Stärkegrad (A, B, C)</th>\n",
       "      <th>Begründung/Kommentar</th>\n",
       "      <th>Annotator</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Semantikerweiterung?</th>\n",
       "      <th>Unersetzlich?</th>\n",
       "      <th>sprachlich irregulär?</th>\n",
       "      <th>pointiert?</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Bei Beobachtung solchen moralischen Wertes ka...</td>\n",
       "      <td>Metapher</td>\n",
       "      <td>das Horoskop stellen</td>\n",
       "      <td>einer Nation</td>\n",
       "      <td>B</td>\n",
       "      <td>Horoskop stellen - bezogen auf Nationen ist da...</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Diese wolle die bittere Auslese, ohne die auc...</td>\n",
       "      <td>Metapher</td>\n",
       "      <td>bittere</td>\n",
       "      <td>Auslese</td>\n",
       "      <td>A</td>\n",
       "      <td>Unauffällig, aber doch metaphorisch: Dass eine...</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Wenn es dem Verfasser gelungen ist, ein gesic...</td>\n",
       "      <td>Metapher</td>\n",
       "      <td>ein gesichertes Fundament und die ersten Pfeil...</td>\n",
       "      <td>die Lösung der hier zur Bearbeitung gestellten...</td>\n",
       "      <td>A</td>\n",
       "      <td>Bruch, Fokus nicht ohne Bedeutungsverlust erse...</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Man kann vielmehr in das Getriebe des gesells...</td>\n",
       "      <td>Metapher</td>\n",
       "      <td>Leuchte der Wissenschaft</td>\n",
       "      <td>in das Getriebe des gesellschaftlichen Lebens</td>\n",
       "      <td>B</td>\n",
       "      <td>Bruch, semantische Erweiterung, nicht ersetzba...</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Wohlfahrt hätte, wäre ein absurdum. Dies komm...</td>\n",
       "      <td>Metapher</td>\n",
       "      <td>fast die ganze Außenwelt lückenlos umspannen</td>\n",
       "      <td>des Menschen Interessen</td>\n",
       "      <td>B</td>\n",
       "      <td>Irritation, unersetzlich, Bedeutungserweiterun...</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>NaN</td>\n",
       "      <td>137\\tAm einfachsten und am schnellsten wird de...</td>\n",
       "      <td>Metapher</td>\n",
       "      <td>['der gordische Knoten', 'dem Schwerte', 'durc...</td>\n",
       "      <td>['dieser Frage', 'des \"frommen Glaubens\"', '',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GoldStandard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>622.0</td>\n",
       "      <td>['Haeckel_Lebenswunder_Stufe2_B.xmi', 'Haeckel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3194</th>\n",
       "      <td>NaN</td>\n",
       "      <td>188\\tIndem ich meine Leser einlade, mit mir da...</td>\n",
       "      <td>Metapher</td>\n",
       "      <td>['der engen Eingangspforte', 'Eintrittskarte',...</td>\n",
       "      <td>['das weite Gebiet der monistischen Philosophi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GoldStandard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>628.0</td>\n",
       "      <td>['Haeckel_Lebenswunder_Stufe2_B.xmi', 'Haeckel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3205</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hat aber die Abstammungslehre recht, so muß si...</td>\n",
       "      <td>Metapher</td>\n",
       "      <td>['keine Violine', 'keine Violine da ist']</td>\n",
       "      <td>['lückenlosen Übergänge', 'um sie zum Ausdruck...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GoldStandard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>639.0</td>\n",
       "      <td>['Haecker_Stufe2_T.xmi', 'Haecker_Stufe2_P.xmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3206</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Lassen sich nach dem zuletztGesagten die Organ...</td>\n",
       "      <td>Metapher</td>\n",
       "      <td>['Damm', 'durchbricht', 'Damm']</td>\n",
       "      <td>['naturwissenschaftlicher Forschnngsgrundsätze...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GoldStandard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640.0</td>\n",
       "      <td>['Haecker_Stufe2_T.xmi', 'Haecker_Stufe2_T.xmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Die Amerikaner folgern, da sie alle Rohmateria...</td>\n",
       "      <td>Metapher</td>\n",
       "      <td>['einer turmhohen Mauer', 'turmhohen Mauer']</td>\n",
       "      <td>['Schutzzöllen', 'von Schutzzöllen']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GoldStandard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>654.0</td>\n",
       "      <td>['Schalk_Stufe2_B_5.xmi', 'Schalk_Stufe2_T_5.x...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>906 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Seite                                         Textstelle Metapher?  \\\n",
       "0      NaN   Bei Beobachtung solchen moralischen Wertes ka...  Metapher   \n",
       "2      NaN   Diese wolle die bittere Auslese, ohne die auc...  Metapher   \n",
       "3      NaN   Wenn es dem Verfasser gelungen ist, ein gesic...  Metapher   \n",
       "7      NaN   Man kann vielmehr in das Getriebe des gesells...  Metapher   \n",
       "9      NaN   Wohlfahrt hätte, wäre ein absurdum. Dies komm...  Metapher   \n",
       "...    ...                                                ...       ...   \n",
       "3188   NaN  137\\tAm einfachsten und am schnellsten wird de...  Metapher   \n",
       "3194   NaN  188\\tIndem ich meine Leser einlade, mit mir da...  Metapher   \n",
       "3205   NaN  Hat aber die Abstammungslehre recht, so muß si...  Metapher   \n",
       "3206   NaN  Lassen sich nach dem zuletztGesagten die Organ...  Metapher   \n",
       "3220   NaN  Die Amerikaner folgern, da sie alle Rohmateria...  Metapher   \n",
       "\n",
       "                                                  Fokus  \\\n",
       "0                                  das Horoskop stellen   \n",
       "2                                               bittere   \n",
       "3     ein gesichertes Fundament und die ersten Pfeil...   \n",
       "7                              Leuchte der Wissenschaft   \n",
       "9          fast die ganze Außenwelt lückenlos umspannen   \n",
       "...                                                 ...   \n",
       "3188  ['der gordische Knoten', 'dem Schwerte', 'durc...   \n",
       "3194  ['der engen Eingangspforte', 'Eintrittskarte',...   \n",
       "3205          ['keine Violine', 'keine Violine da ist']   \n",
       "3206                    ['Damm', 'durchbricht', 'Damm']   \n",
       "3220       ['einer turmhohen Mauer', 'turmhohen Mauer']   \n",
       "\n",
       "                                                 Rahmen Stärkegrad (A, B, C)  \\\n",
       "0                                          einer Nation                    B   \n",
       "2                                               Auslese                    A   \n",
       "3     die Lösung der hier zur Bearbeitung gestellten...                    A   \n",
       "7         in das Getriebe des gesellschaftlichen Lebens                    B   \n",
       "9                               des Menschen Interessen                    B   \n",
       "...                                                 ...                  ...   \n",
       "3188  ['dieser Frage', 'des \"frommen Glaubens\"', '',...                  NaN   \n",
       "3194  ['das weite Gebiet der monistischen Philosophi...                  NaN   \n",
       "3205  ['lückenlosen Übergänge', 'um sie zum Ausdruck...                  NaN   \n",
       "3206  ['naturwissenschaftlicher Forschnngsgrundsätze...                  NaN   \n",
       "3220               ['Schutzzöllen', 'von Schutzzöllen']                  NaN   \n",
       "\n",
       "                                   Begründung/Kommentar     Annotator  \\\n",
       "0     Horoskop stellen - bezogen auf Nationen ist da...             B   \n",
       "2     Unauffällig, aber doch metaphorisch: Dass eine...             B   \n",
       "3     Bruch, Fokus nicht ohne Bedeutungsverlust erse...             B   \n",
       "7     Bruch, semantische Erweiterung, nicht ersetzba...             B   \n",
       "9     Irritation, unersetzlich, Bedeutungserweiterun...             B   \n",
       "...                                                 ...           ...   \n",
       "3188                                                NaN  GoldStandard   \n",
       "3194                                                NaN  GoldStandard   \n",
       "3205                                                NaN  GoldStandard   \n",
       "3206                                                NaN  GoldStandard   \n",
       "3220                                                NaN  GoldStandard   \n",
       "\n",
       "     Unnamed: 2 Semantikerweiterung? Unersetzlich? sprachlich irregulär?  \\\n",
       "0           NaN                  NaN           NaN                   NaN   \n",
       "2           NaN                  NaN           NaN                   NaN   \n",
       "3           NaN                  NaN           NaN                   NaN   \n",
       "7           NaN                  NaN           NaN                   NaN   \n",
       "9           NaN                  NaN           NaN                   NaN   \n",
       "...         ...                  ...           ...                   ...   \n",
       "3188        NaN                  NaN           NaN                   NaN   \n",
       "3194        NaN                  NaN           NaN                   NaN   \n",
       "3205        NaN                  NaN           NaN                   NaN   \n",
       "3206        NaN                  NaN           NaN                   NaN   \n",
       "3220        NaN                  NaN           NaN                   NaN   \n",
       "\n",
       "     pointiert?  Unnamed: 0                                           Filename  \n",
       "0           NaN         NaN                                                NaN  \n",
       "2           NaN         NaN                                                NaN  \n",
       "3           NaN         NaN                                                NaN  \n",
       "7           NaN         NaN                                                NaN  \n",
       "9           NaN         NaN                                                NaN  \n",
       "...         ...         ...                                                ...  \n",
       "3188        NaN       622.0  ['Haeckel_Lebenswunder_Stufe2_B.xmi', 'Haeckel...  \n",
       "3194        NaN       628.0  ['Haeckel_Lebenswunder_Stufe2_B.xmi', 'Haeckel...  \n",
       "3205        NaN       639.0  ['Haecker_Stufe2_T.xmi', 'Haecker_Stufe2_P.xmi...  \n",
       "3206        NaN       640.0  ['Haecker_Stufe2_T.xmi', 'Haecker_Stufe2_T.xmi...  \n",
       "3220        NaN       654.0  ['Schalk_Stufe2_B_5.xmi', 'Schalk_Stufe2_T_5.x...  \n",
       "\n",
       "[906 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading annotated data of the gold standard\n",
    "gold_standard_df = pd.read_csv(ROOT_PATH + DATA_PATH + '/Annotationen-Stufe-2-GoldStandard.csv')\n",
    "\n",
    "# Loading annotated data for non-metaphors\n",
    "no_metaphor_df = pd.read_csv(ROOT_PATH + DATA_PATH + '/NoMetaphor.csv', index_col=0)\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "raw_df = pd.concat([raw_df, gold_standard_df, no_metaphor_df], axis=0,ignore_index=True)\n",
    "\n",
    "# If the related parameter is set to true and a valid Deepl API Key is present, oversampling will be performed on the raw_df and this oversampled_data_df will be saved as .csv file\n",
    "if (oversample_dataframe and deepl_auth_key != 'DEEPL_AUTH_KEY'):\n",
    "  oversampled_data_df = oversample_dataframe(gold_standard_df)\n",
    "  oversampled_data_df.to_csv('goldstandard_dataframe_oversampled.csv', index=False)\n",
    "\n",
    "display(raw_df[raw_df['Metapher?'] == 'Metapher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45821348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotators = ['T', 'A', 'P', 'B', 'K']\n",
    "def get_class_data_by_annotator(annotator = 'all', class_name = 'Metapher', size= 0)-> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Gets the class data by annotator\n",
    "  :param annotator: the specific annotator to data from. If 'all', data from all annotators is returned\n",
    "  :param class_name: The name of the class which should be returned\n",
    "  :param size: The size of the returned dataframe. If size=0, all data is returned\n",
    "  :return df: A dataframe of given size, containing all data of a given annotator and class\n",
    "  \"\"\"  \n",
    "  annotators = set(raw_df['Annotator'].tolist())\n",
    "  if annotator == 'all':\n",
    "    df = raw_df.loc[(raw_df['Annotator'].isin(['T', 'A', 'P', 'B', 'K', 'GoldStandard', 'X']) ) & (raw_df['Metapher?'] == class_name)]\n",
    "  elif annotator not in annotators:\n",
    "    raise Exception('The given Annotator is not found')\n",
    "  else:\n",
    "    df = raw_df.loc[(raw_df['Annotator'] == annotator) & (raw_df['Metapher?'] == class_name)]\n",
    "\n",
    "  # shuffle rows\n",
    "  df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "  if size != 0:\n",
    "    df = df.head(size)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7342d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(train_texts, test_texts, train_labels, test_labels, strategy=\"most_frequent\"):\n",
    "  \"\"\"\n",
    "  Returns the Accuracy, Average Macro F1 score and individual Macro F1 score for each class for a given set of train and test data with a given \n",
    "  :param train_texts: The train set\n",
    "  :param test_texts: The test set\n",
    "  :param train_labels: The train labels\n",
    "  :param test_labels: The test labels\n",
    "  :param strategy: The strategy for the classifier (for example 'most_frequent', 'stratified')\n",
    "  :return acc: The accuracy of the baseline classifier on the given data with the given strategy\n",
    "  :return macro_f1: The average Macro F1 score of the baseline classifier on the given data with the given strategy\n",
    "  :return per_class_macro_f1: The individual class Macro F1 scores of the baseline classifier on the given data with the given strategy\n",
    "  \"\"\"  \n",
    "  dummy_clf = DummyClassifier(strategy=strategy)\n",
    "  dummy_clf.fit(train_texts, train_labels)\n",
    "  prediction = dummy_clf.predict(test_texts)\n",
    "  # calculate accuracy using sklearn's function\n",
    "  test_labels = test_labels.to_numpy(dtype=int)\n",
    "  prediction = prediction.astype(int)\n",
    "  acc = accuracy_score(test_labels, prediction)\n",
    "  macro_f1 = f1_score(test_labels, prediction, average='macro')\n",
    "  per_class_macro_f1 = f1_score(test_labels, prediction, average=None).tolist()\n",
    "\n",
    "  display(\"-----------------------------------\")\n",
    "  display(\"Baseline using strategy=\", strategy)\n",
    "  display(\"Accuracy:\", acc)\n",
    "  display(\"Macro F1:\", macro_f1)\n",
    "  display(\"Per Class Macro F1:\", per_class_macro_f1)\n",
    "  display(\"-----------------------------------\")\n",
    "\n",
    "  return acc, macro_f1, per_class_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8288402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_tokenizer(model_type='redewiedergabe', smote=False):\n",
    "  \"\"\"\n",
    "  Returns an individual model and corresponding tokenizer \n",
    "  :param model_type: The type of model to load\n",
    "  :param smote: The test set\n",
    "  :return model: The average Macro F1 score of the baseline classifier on the given data with the given strategy\n",
    "  :return tokenizer: The individual class Macro F1 scores of the baseline classifier on the given data with the given strategy\n",
    "  \"\"\"    \n",
    "\n",
    "  if model_type == 'fine_tuned':\n",
    "    if smote>0:\n",
    "      model = BertForSequenceClassificationSMOTE.from_pretrained(ROOT_PATH + MODEL_PATH, cache_dir=None, num_labels=3)\n",
    "    else:\n",
    "      model = BertForSequenceClassification.from_pretrained(ROOT_PATH + MODEL_PATH, cache_dir=None, num_labels=3)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ROOT_PATH + MODEL_PATH)\n",
    "  elif model_type == 'intermediate_task_vua':\n",
    "    if smote>0:\n",
    "      model = BertForSequenceClassificationSMOTE.from_pretrained(ROOT_PATH + '/intermediate-task-vua/model', num_labels=3, ignore_mismatched_sizes=True)\n",
    "    else:\n",
    "      model = BertForSequenceClassification.from_pretrained(ROOT_PATH + '/intermediate-task-vua/model', num_labels=3, ignore_mismatched_sizes=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ROOT_PATH + '/intermediate-task-trofi-vua/model')\n",
    "  elif model_type == 'intermediate_task_trofi':\n",
    "    if smote>0:\n",
    "      model = BertForSequenceClassificationSMOTE.from_pretrained(ROOT_PATH + '/intermediate-task-trofi/model', num_labels=3, ignore_mismatched_sizes=True)\n",
    "    else:\n",
    "      model = BertForSequenceClassification.from_pretrained(ROOT_PATH + '/intermediate-task-trofi/model', num_labels=3, ignore_mismatched_sizes=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ROOT_PATH + '/intermediate-task-trofi/model')\n",
    "  elif model_type == 'bert_base_multilingual_cased':\n",
    "    if smote:\n",
    "      model = BertForSequenceClassificationSMOTE.from_pretrained(\"bert-base-multilingual-cased\", cache_dir=None, num_labels=3)\n",
    "    else:\n",
    "      model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", cache_dir=None, num_labels=3)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "  elif model_type == 'redewiedergabe':\n",
    "    if smote:\n",
    "      model = BertForSequenceClassificationSMOTE.from_pretrained(\"redewiedergabe/bert-base-historical-german-rw-cased\", cache_dir=None, num_labels=3)\n",
    "    else:\n",
    "      model = BertForSequenceClassification.from_pretrained(\"redewiedergabe/bert-base-historical-german-rw-cased\", cache_dir=None, num_labels=3)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"redewiedergabe/bert-base-historical-german-rw-cased\")\n",
    "  return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a49393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaphorDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    The dataset class for metaphors\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"\n",
    "        Initializes the dataset\n",
    "        :param encodings: The type of model to load\n",
    "        :param labels: Boolean value to toggle SMOTE\n",
    "        \"\"\"    \n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns an individual item by id\n",
    "        :param idx: The id of the item to return\n",
    "        :return item: The chosen item\n",
    "        \"\"\"    \n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Helper to return the size of the dataset\n",
    "        :return lenght: the size of the dataset\n",
    "        \"\"\"    \n",
    "        return len(self.labels)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Computes accuracy, macro_f1 score and individual macro f1 per class for a given prediction\n",
    "    :param pred: The prediction\n",
    "    :return dict: A dictionary containing accuracy, macro_f1 score and individual macro f1 per class\n",
    "    \"\"\"    \n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # calculate accuracy using sklearn's function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "    per_class_macro_f1 = f1_score(labels, preds, average=None).tolist()\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'macro_f1': macro_f1,\n",
    "        'per_class_macro_f1': per_class_macro_f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8cfefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(path, results, annotator):\n",
    "  \"\"\"\n",
    "  Saves the results of a training to a given path\n",
    "  :param path: The path to save to\n",
    "  :param results: The results to save\n",
    "  :param annotator: The annotator of this particular result\n",
    "  \"\"\"    \n",
    "  print(\"Saving results to\", path)\n",
    "  with open(path + '/results_'+annotator+'.csv', 'w') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in results.items():\n",
    "       writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b60756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_labels(df):  \n",
    "  \"\"\"\n",
    "  Returns the labels as numerical values: '0' for Non-Metaphors, '1' for Metaphor candidates and '2' for Metaphors\n",
    "  :param df: The dataframe to change\n",
    "  :return df: The changed dataframe, containing labels as numerical values\n",
    "  \"\"\" \n",
    "  df.loc[df['Metapher?'] == 'Nein', 'Metapher?'] = 0\n",
    "  df.loc[df['Metapher?'] == 'Metaphernkandidat', 'Metapher?'] = 1\n",
    "  df.loc[df['Metapher?'] == 'Unklar', 'Metapher?'] = 1\n",
    "  df.loc[df['Metapher?'] == 'Grenzfall', 'Metapher?'] = 1\n",
    "  df.loc[df['Metapher?'] == 'Metapher', 'Metapher?'] = 2\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed26586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oversamples(entry):\n",
    "  \"\"\"\n",
    "  Returns the oversampling entries corresponding to a specific entry\n",
    "  :param entry: The entry to get the oversampling data to\n",
    "  :return oversamples: The oversampling data to the given entry\n",
    "  \"\"\" \n",
    "  oversamples = pd.DataFrame()\n",
    "  textstelle = entry['Textstelle']\n",
    "  # getting oversampling data from gold standard\n",
    "  gold_standard_entry = get_numerical_labels(gold_standard_df[gold_standard_df['Textstelle']==textstelle])\n",
    "  for language in [\"Synonym (aus Englischem)\", \"Synonym (aus Spanischem)\", \"Synonym (aus Tchechischem)\", \"Synonym (aus Polnischem)\"]:\n",
    "    oversamples= oversamples.append(gold_standard_entry)\n",
    "    oversamples.iloc[-1]['Textstelle'] = oversamples.iloc[-1][language]\n",
    "  return oversamples\n",
    "\n",
    "def generate_oversampled_data(df, size):\n",
    "  \"\"\"\n",
    "  Generates oversampled data to given data with a specific size by randomly selecting from all oversampling data\n",
    "  :param df: The data for which oversampling data should be generated\n",
    "  :param size: The size of the generated data\n",
    "  :return new_samples: The oversampling data to the given data with given size\n",
    "  \"\"\"   \n",
    "  new_samples = pd.DataFrame()\n",
    "  for index, row in df.iterrows():\n",
    "    new_samples= new_samples.append(get_oversamples(row))\n",
    "  return new_samples.sample(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97b3367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_statistics(df):\n",
    "    \"\"\"\n",
    "    Generates a statistic of a given dataframe\n",
    "    :param df: The data for which a statistic should be generated\n",
    "    :return statistic: The statistic of the given dataframe\n",
    "    \"\"\"   \n",
    "    return \"non-metaphors: {} ({:.0%}), metaphor candidates: {} ({:.0%}), metaphors: {} ({:.0%})\".format(df['Metapher?'].value_counts()[0],df['Metapher?'].value_counts()[0]/df['Metapher?'].value_counts().sum(),df['Metapher?'].value_counts()[1],df['Metapher?'].value_counts()[1]/df['Metapher?'].value_counts().sum(),df['Metapher?'].value_counts()[2],df['Metapher?'].value_counts()[2]/df['Metapher?'].value_counts().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48453d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the maximum sentence length\n",
    "max_length = 512\n",
    "# setting the default stärkegrad of the raw data\n",
    "raw_df = set_default_staerkegrad_df(raw_df)\n",
    "\n",
    "def train(annotator, model_type, epochs=3, folds=10, oversampling=True, smote=0, path='/'):\n",
    "  \"\"\"\n",
    "  Training a given model with k-fold cross-validation and various oversampling strategies\n",
    "  The resulting metrics are saved as CSV to a given path\n",
    "  :param annotator: The annotator on whose data to train\n",
    "  :param model_type: The type of model to use\n",
    "  :param epochs: The amount of epochs to train\n",
    "  :param folds: The amount of folds to use for k-fold cross validation\n",
    "  :param oversampling: Boolean to set true if oversampling should be used\n",
    "  :param smote: K value for smote. If set to 0, smote is deactivated\n",
    "  :param path: The path to save the results to\n",
    "  :return evaluation_results: The results of the evaluation of the trained model\n",
    "  \"\"\"     \n",
    "  print(\"Training with data from annotator\", annotator)\n",
    "  metaphor_candidates = get_class_data_by_annotator(annotator, class_name='Metaphernkandidat')\n",
    "  metaphors = get_class_data_by_annotator(annotator, class_name='Metapher')\n",
    "  # Using only the same amount of non-metaphors as metaphor candidates\n",
    "  not_metaphors = get_class_data_by_annotator(class_name='Nein', size=len(metaphor_candidates))\n",
    "\n",
    "  df = pd.concat([metaphors, metaphor_candidates, not_metaphors])\n",
    "  # Shuffle the rows  \n",
    "  df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "  # Data Statistics\n",
    "  samples = len(metaphor_candidates)+len(metaphors)+len(not_metaphors)\n",
    "  print(\"Number of individual classes in data:\", df_statistics(df))\n",
    "\n",
    "  # changing the labels to numerical values\n",
    "  df = get_numerical_labels(df)\n",
    "\n",
    "  baseline_accuracy = []\n",
    "  baseline_macro_f1 = []\n",
    "  baseline_per_class_macro_f1 = []\n",
    "\n",
    "  evaluation_accuracy = []\n",
    "  evaluation_macro_f1 = []\n",
    "  evaluation_per_class_macro_f1 = []\n",
    "\n",
    "\n",
    "  # K-Fold Cross Validation\n",
    "  kf = StratifiedKFold(n_splits=folds, shuffle=False)\n",
    "  i = 0\n",
    "  for train_index, test_index in kf.split(df['Textstelle'], df['Metapher?'].to_list()):\n",
    "    \n",
    "    model, tokenizer = load_model_tokenizer(model_type=model_type, smote=smote)\n",
    "    i += 1\n",
    "    display(\"-----------------------------------\")\n",
    "    print(\"Annotator\", annotator, \"| Fold #\", i)\n",
    "\n",
    "    train_samples = df.iloc[train_index.tolist()]\n",
    "    test_samples = df.iloc[test_index.tolist()]\n",
    "\n",
    "    train_metaphors = train_samples[train_samples[\"Metapher?\"] == 2]\n",
    "    test_metaphors = test_samples[test_samples[\"Metapher?\"] == 2]\n",
    "\n",
    "    #print(\"Number of individual classes in train:\", df_statistics(train_samples))\n",
    "    #print(\"Number of individual classes in test:\", df_statistics(test_samples))\n",
    "\n",
    "    if oversampling:\n",
    "      # add oversampled metaphors\n",
    "      oversampled__train_samples = generate_oversampled_data(df=train_metaphors, size=(train_samples['Metapher?'].value_counts()[0]-train_samples['Metapher?'].value_counts()[2]))\n",
    "\n",
    "      train_samples = pd.concat([train_samples, oversampled__train_samples]) \n",
    "\n",
    "    #print(\"Number of individual classes in train (after oversampling):\", df_statistics(train_samples))\n",
    "    #print(\"Number of individual classes in test (after oversampling):\", df_statistics(test_samples))\n",
    "\n",
    "\n",
    "    train_texts = train_samples['Textstelle']\n",
    "    test_texts = test_samples['Textstelle']\n",
    "    train_labels = train_samples['Metapher?']\n",
    "    test_labels = test_samples['Metapher?']\n",
    "\n",
    "    # Baseline\n",
    "    b_acc, b_macro_f1, b_per_class_macro_f1 = baseline(train_texts, test_texts, train_labels, test_labels, strategy=\"stratified\") # use 'most_frequent' 'stratified' or 'uniform'\n",
    "\n",
    "    baseline_accuracy.append(b_acc)\n",
    "    baseline_macro_f1.append(b_macro_f1)\n",
    "    baseline_per_class_macro_f1.append(b_per_class_macro_f1)\n",
    "    \n",
    "    train_encodings = tokenizer(train_texts.to_list(), truncation=True, padding=True, max_length=max_length)\n",
    "    test_encodings = tokenizer(test_texts.to_list(), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "\n",
    "    # convert our tokenized data into a torch Dataset\n",
    "    train_dataset = MetaphorDataset(train_encodings, train_labels.tolist())\n",
    "    test_dataset = MetaphorDataset(test_encodings, test_labels.tolist())\n",
    "\n",
    "    if smote>0:\n",
    "      get_smote_points(model, train_samples['Metapher?'].value_counts()[2], train_dataset, test_dataset)\n",
    "      model.smote_factor = int(train_samples['Metapher?'].value_counts()[0]/train_samples['Metapher?'].value_counts()[2])-1\n",
    "      model.smote_k = smote\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=epochs,              # total number of training epochs\n",
    "        per_device_train_batch_size=8,  # batch size per device during training\n",
    "        per_device_eval_batch_size=20,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
    "        # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "        save_total_limit=1,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",     # evaluate each `logging_steps`\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=test_dataset,          # evaluation dataset\n",
    "        compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    #print(\"SMOTE created\", model.smote_counter, \"synthetic data points\")\n",
    "\n",
    "    evaluation = trainer.evaluate()\n",
    "\n",
    "    evaluation_accuracy.append(evaluation['eval_accuracy'])\n",
    "    evaluation_macro_f1.append(evaluation['eval_macro_f1'])\n",
    "    evaluation_per_class_macro_f1.append(evaluation['eval_per_class_macro_f1'])\n",
    "  \n",
    "  evaluation_results = {\n",
    "    \"baseline_accuracy\": baseline_accuracy,\n",
    "    \"baseline_macro_f1\": baseline_macro_f1,\n",
    "    \"baseline_per_class_macro_f1\": baseline_per_class_macro_f1,\n",
    "    \"evaluation_accuracy\": evaluation_accuracy,\n",
    "    \"evaluation_macro_f1\": evaluation_macro_f1,\n",
    "    \"evaluation_per_class_macro_f1\": evaluation_per_class_macro_f1\n",
    "  }\n",
    "\n",
    "  try:\n",
    "    save_results(path,evaluation_results, annotator)\n",
    "  except:\n",
    "    print(\"There seems to be a problem with Google Drive. The CSV could not be saved. Results are only stored locally in results[annotator].\")\n",
    "  return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4751b415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fynn/Uni/DL4NLP/results/runs/2022-07-28-20:42:03 | redewiedergabe | EPOCHS 3 | FOLDS 10\n",
      "Training with data from annotator GoldStandard\n",
      "Number of individual classes in data: non-metaphors: 529 (45%), metaphor candidates: 529 (45%), metaphors: 129 (11%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotator GoldStandard | Fold # 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Baseline using strategy='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stratified'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.40336134453781514"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Macro F1:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3393545163456668"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Per Class Macro F1:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.40404040404040403, 0.46017699115044247, 0.15384615384615385]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fynn/Uni/.venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1068\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 402\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8733, 'learning_rate': 1.3400000000000002e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.8130081300813008, 0.7058823529411764, 0.0]\" of type <class 'list'> for key \"eval/per_class_macro_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to ./results/checkpoint-134\n",
      "Configuration saved in ./results/checkpoint-134/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.706839919090271, 'eval_accuracy': 0.7226890756302521, 'eval_macro_f1': 0.5062968276741591, 'eval_per_class_macro_f1': [0.8130081300813008, 0.7058823529411764, 0.0], 'eval_runtime': 13.4334, 'eval_samples_per_second': 8.859, 'eval_steps_per_second': 0.447, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-134/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-135] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5591, 'learning_rate': 2.6800000000000004e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.8103448275862069, 0.7522935779816513, 0.0]\" of type <class 'list'> for key \"eval/per_class_macro_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to ./results/checkpoint-268\n",
      "Configuration saved in ./results/checkpoint-268/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6681127548217773, 'eval_accuracy': 0.7394957983193278, 'eval_macro_f1': 0.5208794685226193, 'eval_per_class_macro_f1': [0.8103448275862069, 0.7522935779816513, 0.0], 'eval_runtime': 13.1707, 'eval_samples_per_second': 9.035, 'eval_steps_per_second': 0.456, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-268/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-45] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3953, 'learning_rate': 4.02e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.8224299065420562, 0.7457627118644068, 0.0]\" of type <class 'list'> for key \"eval/per_class_macro_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to ./results/checkpoint-402\n",
      "Configuration saved in ./results/checkpoint-402/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8085532188415527, 'eval_accuracy': 0.7394957983193278, 'eval_macro_f1': 0.5227308728021544, 'eval_per_class_macro_f1': [0.8224299065420562, 0.7457627118644068, 0.0], 'eval_runtime': 13.3198, 'eval_samples_per_second': 8.934, 'eval_steps_per_second': 0.45, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-402/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-134] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-402 (score: 0.5227308728021544).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 977.4118, 'train_samples_per_second': 3.278, 'train_steps_per_second': 0.411, 'train_loss': 0.6092520756507988, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.8224299065420562, 0.7457627118644068, 0.0]\" of type <class 'list'> for key \"eval/per_class_macro_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8085532188415527, 'eval_accuracy': 0.7394957983193278, 'eval_macro_f1': 0.5227308728021544, 'eval_per_class_macro_f1': [0.8224299065420562, 0.7457627118644068, 0.0], 'eval_runtime': 13.2361, 'eval_samples_per_second': 8.991, 'eval_steps_per_second': 0.453, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/config.json from cache at /Users/fynn/.cache/huggingface/transformers/7af39f301143e3a47aeff7ff49209199d0a40bcc26e1a4b00085cbafea391142.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/pytorch_model.bin from cache at /Users/fynn/.cache/huggingface/transformers/0c5441d91b4733bae33040c3d347f0a937b98e4c55d57d9de23a320f823dc7b6.f0dcfc76f94cc42435d76f25082f8ee1e53917866a4d56cad2e646578f6d905f\n",
      "Some weights of the model checkpoint at redewiedergabe/bert-base-historical-german-rw-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at redewiedergabe/bert-base-historical-german-rw-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/config.json from cache at /Users/fynn/.cache/huggingface/transformers/7af39f301143e3a47aeff7ff49209199d0a40bcc26e1a4b00085cbafea391142.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"redewiedergabe/bert-base-historical-german-rw-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/vocab.txt from cache at /Users/fynn/.cache/huggingface/transformers/79f3661d828f126c7d5e5e252de4fbb11431591bda723e8ce822c940b889ffd0.1a19280a6fcc33d00703aaae5e447798a91de37cbc7327384000411a3b816392\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/added_tokens.json from cache at /Users/fynn/.cache/huggingface/transformers/3de4dd23d4fbaccfe4a4a9ff8e30dcc0ae52d87732a5db53394bc06d119a0e98.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/special_tokens_map.json from cache at /Users/fynn/.cache/huggingface/transformers/74740ba17e63e288486b0a161bf6d032c5dbb937124b0aebf8d36e87ecc5d04e.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/tokenizer_config.json from cache at /Users/fynn/.cache/huggingface/transformers/e356dbca6a3d07f43542faac2ac8c7deded4baba3cca7091dc09fd0bf57db109.6391beef2ceed2cdba47401eb12680200856c97d2f2b56143e515d7c0f36a66a\n",
      "loading configuration file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/config.json from cache at /Users/fynn/.cache/huggingface/transformers/7af39f301143e3a47aeff7ff49209199d0a40bcc26e1a4b00085cbafea391142.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"redewiedergabe/bert-base-historical-german-rw-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/config.json from cache at /Users/fynn/.cache/huggingface/transformers/7af39f301143e3a47aeff7ff49209199d0a40bcc26e1a4b00085cbafea391142.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"redewiedergabe/bert-base-historical-german-rw-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotator GoldStandard | Fold # 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Baseline using strategy='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stratified'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3865546218487395"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Macro F1:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3083245149911817"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Per Class Macro F1:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.4190476190476191, 0.42592592592592593, 0.08]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/fynn/Uni/.venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1068\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 402\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='402' max='402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [402/402 37:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Per Class Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.998200</td>\n",
       "      <td>0.633032</td>\n",
       "      <td>0.722689</td>\n",
       "      <td>0.507214</td>\n",
       "      <td>[0.823529411764706, 0.6981132075471698, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.583800</td>\n",
       "      <td>0.551735</td>\n",
       "      <td>0.798319</td>\n",
       "      <td>0.563106</td>\n",
       "      <td>[0.8928571428571428, 0.7964601769911505, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.539158</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.570456</td>\n",
       "      <td>[0.8965517241379309, 0.8148148148148149, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 20\n",
      "Trainer is attempting to log a value of \"[0.823529411764706, 0.6981132075471698, 0.0]\" of type <class 'list'> for key \"eval/per_class_macro_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to ./results/checkpoint-134\n",
      "Configuration saved in ./results/checkpoint-134/config.json\n",
      "Model weights saved in ./results/checkpoint-134/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-268] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 20\n",
      "Trainer is attempting to log a value of \"[0.8928571428571428, 0.7964601769911505, 0.0]\" of type <class 'list'> for key \"eval/per_class_macro_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to ./results/checkpoint-268\n",
      "Configuration saved in ./results/checkpoint-268/config.json\n",
      "Model weights saved in ./results/checkpoint-268/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-402] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 20\n",
      "Trainer is attempting to log a value of \"[0.8965517241379309, 0.8148148148148149, 0.0]\" of type <class 'list'> for key \"eval/per_class_macro_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to ./results/checkpoint-402\n",
      "Configuration saved in ./results/checkpoint-402/config.json\n",
      "Model weights saved in ./results/checkpoint-402/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-134] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-402 (score: 0.5704555129842487).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.8965517241379309, 0.8148148148148149, 0.0]\" of type <class 'list'> for key \"eval/per_class_macro_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "loading configuration file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/config.json from cache at /Users/fynn/.cache/huggingface/transformers/7af39f301143e3a47aeff7ff49209199d0a40bcc26e1a4b00085cbafea391142.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/pytorch_model.bin from cache at /Users/fynn/.cache/huggingface/transformers/0c5441d91b4733bae33040c3d347f0a937b98e4c55d57d9de23a320f823dc7b6.f0dcfc76f94cc42435d76f25082f8ee1e53917866a4d56cad2e646578f6d905f\n",
      "Some weights of the model checkpoint at redewiedergabe/bert-base-historical-german-rw-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at redewiedergabe/bert-base-historical-german-rw-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/config.json from cache at /Users/fynn/.cache/huggingface/transformers/7af39f301143e3a47aeff7ff49209199d0a40bcc26e1a4b00085cbafea391142.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"redewiedergabe/bert-base-historical-german-rw-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/vocab.txt from cache at /Users/fynn/.cache/huggingface/transformers/79f3661d828f126c7d5e5e252de4fbb11431591bda723e8ce822c940b889ffd0.1a19280a6fcc33d00703aaae5e447798a91de37cbc7327384000411a3b816392\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/added_tokens.json from cache at /Users/fynn/.cache/huggingface/transformers/3de4dd23d4fbaccfe4a4a9ff8e30dcc0ae52d87732a5db53394bc06d119a0e98.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/special_tokens_map.json from cache at /Users/fynn/.cache/huggingface/transformers/74740ba17e63e288486b0a161bf6d032c5dbb937124b0aebf8d36e87ecc5d04e.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/tokenizer_config.json from cache at /Users/fynn/.cache/huggingface/transformers/e356dbca6a3d07f43542faac2ac8c7deded4baba3cca7091dc09fd0bf57db109.6391beef2ceed2cdba47401eb12680200856c97d2f2b56143e515d7c0f36a66a\n",
      "loading configuration file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/config.json from cache at /Users/fynn/.cache/huggingface/transformers/7af39f301143e3a47aeff7ff49209199d0a40bcc26e1a4b00085cbafea391142.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"redewiedergabe/bert-base-historical-german-rw-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/config.json from cache at /Users/fynn/.cache/huggingface/transformers/7af39f301143e3a47aeff7ff49209199d0a40bcc26e1a4b00085cbafea391142.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"redewiedergabe/bert-base-historical-german-rw-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotator GoldStandard | Fold # 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Baseline using strategy='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stratified'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.40336134453781514"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Macro F1:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.32067019400352736"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Per Class Macro F1:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.4190476190476191, 0.46296296296296297, 0.08]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/fynn/Uni/.venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1068\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 402\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='402' max='402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [402/402 43:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Per Class Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.903800</td>\n",
       "      <td>0.723373</td>\n",
       "      <td>0.697479</td>\n",
       "      <td>0.492334</td>\n",
       "      <td>[0.7927927927927928, 0.6842105263157895, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.534500</td>\n",
       "      <td>0.723369</td>\n",
       "      <td>0.722689</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>[0.8, 0.736, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.648050</td>\n",
       "      <td>0.747899</td>\n",
       "      <td>0.527896</td>\n",
       "      <td>[0.8468468468468469, 0.7368421052631579, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 20\n",
      "Trainer is attempting to log a value of \"[0.7927927927927928, 0.6842105263157895, 0.0]\" of type <class 'list'> for key \"eval/per_class_macro_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to ./results/checkpoint-134\n",
      "Configuration saved in ./results/checkpoint-134/config.json\n",
      "Model weights saved in ./results/checkpoint-134/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-268] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 20\n",
      "Trainer is attempting to log a value of \"[0.8, 0.736, 0.0]\" of type <class 'list'> for key \"eval/per_class_macro_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to ./results/checkpoint-268\n",
      "Configuration saved in ./results/checkpoint-268/config.json\n",
      "Model weights saved in ./results/checkpoint-268/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-402] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 20\n",
      "Trainer is attempting to log a value of \"[0.8468468468468469, 0.7368421052631579, 0.0]\" of type <class 'list'> for key \"eval/per_class_macro_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to ./results/checkpoint-402\n",
      "Configuration saved in ./results/checkpoint-402/config.json\n",
      "Model weights saved in ./results/checkpoint-402/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-134] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-402 (score: 0.5278963173700015).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.8468468468468469, 0.7368421052631579, 0.0]\" of type <class 'list'> for key \"eval/per_class_macro_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "loading configuration file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/config.json from cache at /Users/fynn/.cache/huggingface/transformers/7af39f301143e3a47aeff7ff49209199d0a40bcc26e1a4b00085cbafea391142.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/pytorch_model.bin from cache at /Users/fynn/.cache/huggingface/transformers/0c5441d91b4733bae33040c3d347f0a937b98e4c55d57d9de23a320f823dc7b6.f0dcfc76f94cc42435d76f25082f8ee1e53917866a4d56cad2e646578f6d905f\n",
      "Some weights of the model checkpoint at redewiedergabe/bert-base-historical-german-rw-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at redewiedergabe/bert-base-historical-german-rw-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/config.json from cache at /Users/fynn/.cache/huggingface/transformers/7af39f301143e3a47aeff7ff49209199d0a40bcc26e1a4b00085cbafea391142.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"redewiedergabe/bert-base-historical-german-rw-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/vocab.txt from cache at /Users/fynn/.cache/huggingface/transformers/79f3661d828f126c7d5e5e252de4fbb11431591bda723e8ce822c940b889ffd0.1a19280a6fcc33d00703aaae5e447798a91de37cbc7327384000411a3b816392\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/added_tokens.json from cache at /Users/fynn/.cache/huggingface/transformers/3de4dd23d4fbaccfe4a4a9ff8e30dcc0ae52d87732a5db53394bc06d119a0e98.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/special_tokens_map.json from cache at /Users/fynn/.cache/huggingface/transformers/74740ba17e63e288486b0a161bf6d032c5dbb937124b0aebf8d36e87ecc5d04e.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/tokenizer_config.json from cache at /Users/fynn/.cache/huggingface/transformers/e356dbca6a3d07f43542faac2ac8c7deded4baba3cca7091dc09fd0bf57db109.6391beef2ceed2cdba47401eb12680200856c97d2f2b56143e515d7c0f36a66a\n",
      "loading configuration file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/config.json from cache at /Users/fynn/.cache/huggingface/transformers/7af39f301143e3a47aeff7ff49209199d0a40bcc26e1a4b00085cbafea391142.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"redewiedergabe/bert-base-historical-german-rw-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/resolve/main/config.json from cache at /Users/fynn/.cache/huggingface/transformers/7af39f301143e3a47aeff7ff49209199d0a40bcc26e1a4b00085cbafea391142.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"redewiedergabe/bert-base-historical-german-rw-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotator GoldStandard | Fold # 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Baseline using strategy='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stratified'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.37815126050420167"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Macro F1:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3224691358024691"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Per Class Macro F1:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.39999999999999997, 0.40740740740740744, 0.16]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/fynn/Uni/.venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1068\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 402\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 35/402 02:05 < 23:19, 0.26 it/s, Epoch 0.25/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m   foldernames\u001b[38;5;241m.\u001b[39mappend(folder_name)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m annotator \u001b[38;5;129;01min\u001b[39;00m ANNOTATORS: \n\u001b[0;32m---> 35\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFOLDS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moversampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOVERSAMPLING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSMOTE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRUN_PATH\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     36\u001b[0m   current_results[annotator] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m     37\u001b[0m   foldernames\u001b[38;5;241m.\u001b[39mappend(folder_name)\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(annotator, model_type, epochs, folds, oversampling, smote, path)\u001b[0m\n\u001b[1;32m     99\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m    100\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,          \u001b[38;5;66;03m# output directory\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39mepochs,              \u001b[38;5;66;03m# total number of training epochs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m     metric_for_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    116\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    117\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                         \u001b[38;5;66;03m# the instantiated Transformers model to be trained\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \u001b[38;5;66;03m# training arguments, defined above\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,     \u001b[38;5;66;03m# the callback that computes metrics of interest\u001b[39;00m\n\u001b[1;32m    122\u001b[0m )\n\u001b[0;32m--> 124\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m#print(\"SMOTE created\", model.smote_counter, \"synthetic data points\")\u001b[39;00m\n\u001b[1;32m    127\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/transformers/trainer.py:1317\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1314\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1316\u001b[0m )\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/transformers/trainer.py:1554\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1554\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1557\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1560\u001b[0m ):\n\u001b[1;32m   1561\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/transformers/trainer.py:2183\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> 2183\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2186\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/transformers/trainer.py:2215\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2214\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1554\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1547\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1554\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1566\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1568\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1017\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1008\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1010\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1011\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1012\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1015\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1016\u001b[0m )\n\u001b[0;32m-> 1017\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1030\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:606\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    597\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    598\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    599\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    604\u001b[0m     )\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 606\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:493\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    483\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    492\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:423\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    415\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    422\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 423\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    433\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:355\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    351\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Uni/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Parameters for Training\n",
    "ANNOTATORS = [\n",
    "    #'T',\n",
    "    #'A',\n",
    "    #'P',\n",
    "    #'B', \n",
    "    #'K',\n",
    "    'GoldStandard'] # specify the annotators on which to train. Possible annotators = ['T', 'A', 'P', 'B', 'K', 'GoldStandard']\n",
    "EPOCHS=3\n",
    "FOLDS=10\n",
    "OVERSAMPLING = False\n",
    "SMOTE = 0 #This defines the factor k for k-nearest-neighbors used by SMOTE. SMOTE is off if set to 0.\n",
    "COMMENT='Unbalanced for all annotators.'\n",
    "#########################\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "current_results = {}\n",
    "SAVE_RESULTS = True\n",
    "foldernames = [] \n",
    "\n",
    "models = ['redewiedergabe'] # possible model_types are 'redewiedergabe', 'bert_base_multilingual_cased', 'fine_tuned', 'intermediate_task_trofi' and 'intermediate_task_vua'\n",
    "for model in models: \n",
    "  folder_name = datetime.today().strftime('%Y-%m-%d-%H:%M:%S') + \" | \" + model + \" | EPOCHS \"+ str(EPOCHS)+ \" | FOLDS \"+ str(FOLDS)+ ((\" | SMOTE (K=\"+str(SMOTE)+\")\") if SMOTE>0 else \"\") + ((\" | OVERSAMPLING\") if OVERSAMPLING>0 else \"\")\n",
    "  RUN_PATH = ROOT_PATH + RESULTS_PATH + '/runs/' + folder_name\n",
    "  print(RUN_PATH)\n",
    "\n",
    "  if SAVE_RESULTS:\n",
    "    print(RUN_PATH)\n",
    "    !mkdir \"$RUN_PATH\"\n",
    "    save_settings(RUN_PATH, ANNOTATORS, EPOCHS, FOLDS, model, OVERSAMPLING, SMOTE, COMMENT)\n",
    "    foldernames.append(folder_name)\n",
    "\n",
    "  for annotator in ANNOTATORS: \n",
    "    result = train(annotator, model, epochs=EPOCHS, folds=FOLDS, oversampling=OVERSAMPLING, smote=SMOTE, path=RUN_PATH) \n",
    "    current_results[annotator] = result\n",
    "    foldernames.append(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6746fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_csv(path):\n",
    "  \"\"\"\n",
    "  Loads all CSV data for plotting\n",
    "  :param path: The path to load from\n",
    "  :return df: The loaded data as a dataframe\n",
    "  \"\"\"\n",
    "  df = pd.read_csv(path, names=['measure', 'results'], header=None)\n",
    "  annotator = path[-5]\n",
    "  \n",
    "  df['results'] = df['results'].apply(lambda x: np.array(json.loads(x)))\n",
    "  df['annotator'] = annotator\n",
    "  return df\n",
    "\n",
    "# Plotting with multiple annotators\n",
    "def open_csv_annotators(path, annotator):\n",
    "  \"\"\"\n",
    "  Loads all CSV data for plotting for individual annotators\n",
    "  :param path: The path to load from\n",
    "  :return df: The loaded data as a dataframe\n",
    "  \"\"\"\n",
    "  print(path)\n",
    "  try:\n",
    "    df = pd.read_csv(path, names=['measure', 'results'], header=None)\n",
    "    df['results'] = df['results'].apply(lambda x: np.fromstring(x[1:-1], sep=', '))\n",
    "    df['annotator'] = annotator\n",
    "    annotator_names.append(annotator)\n",
    "  except:\n",
    "    df = pd.DataFrame()\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b212bf2b",
   "metadata": {},
   "source": [
    "We can see that the Bert Model is unable to identify metaphors if it is not trained on backtranslations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
